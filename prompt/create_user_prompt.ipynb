{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# 사용자 데이터프레임 로드 (예시 파일 경로 사용)\n",
    "# tsv 파일을 DataFrame으로 읽기\n",
    "user = pd.read_csv('user.tsv', sep='\\t', names=['User', 'History', 'Train', 'Test'])\n",
    "user_raw = pd.read_csv('user(raw).tsv', sep='\\t',  names=['User', 'History', 'Train', 'Test'])\n",
    "\n",
    "history_news = pd.read_csv('history/news.tsv', sep='\\t',  names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'])\n",
    "\n",
    "train_news = pd.read_csv('train/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'])\n",
    "train_behaviors = pd.read_csv('train/behaviors.tsv', sep='\\t', names=['User ID', 'Click time', 'Click history', 'click'])\n",
    "\n",
    "test_news = pd.read_csv('test/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'])\n",
    "test_behaviors = pd.read_csv('test/behaviors.tsv', sep='\\t', names=['User ID', 'Click time', 'Click history', 'click'])\n",
    "\n",
    "# publish 순서에 맞게 오름차순으로 정렬\n",
    "history_news_sorted = history_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# publish 순서에 맞게 오름차순으로 정렬\n",
    "train_news_sorted = train_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Negative Prompt 생성 및 token 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Input Tokens: 3192670\n",
      "Total Output Tokens: 243359\n"
     ]
    }
   ],
   "source": [
    "# 사용자별 input과 output 데이터를 저장할 딕셔너리\n",
    "user_data = {}\n",
    "user_metadata = []\n",
    "hidden_positions_data = []\n",
    "\n",
    "# 사용할 gpt (gpt별 사용할 token 계산용도)\n",
    "gpt_name = \"gpt-3.5-turbo\"\n",
    "# gpt_name = \"gpt-4o-mini\"\n",
    "\n",
    "# 사용자 ID 범위 설정\n",
    "user_count = 1000\n",
    "user_ids = [f'U{i}' for i in range(1, user_count + 1)]\n",
    "\n",
    "for ID in user_ids:\n",
    "    history = user[user['User'] == ID]['History'].iloc[0]\n",
    "\n",
    "    # History에서 News ID 추출\n",
    "    news_ids = []\n",
    "    [news_ids.append(entry.split(',')[0]) for entry in history.split(';') if entry and entry.split(',')[0] not in news_ids]\n",
    "\n",
    "    # News ID의 맞는 Title 추출\n",
    "    titles = []\n",
    "    for news_id in news_ids:\n",
    "        matching_rows = history_news[history_news['News ID'] == news_id]\n",
    "        if not matching_rows.empty:\n",
    "            titles.append(matching_rows.iloc[0]['Title'])\n",
    "\n",
    "    train = user[user['User'] == ID]['Train'].iloc[0]\n",
    "\n",
    "    # Train에서 News ID 추출\n",
    "    train_ids = []\n",
    "    [train_ids.append(entry.split(',')[0]) for entry in train.split(';') if entry and entry.split(',')[0] not in train_ids]\n",
    "\n",
    "    # News ID의 맞는 Title 추출\n",
    "    train_titles = []\n",
    "    for train_id in train_ids:\n",
    "        matching_rows = train_news[train_news['News ID'] == train_id]\n",
    "        if not matching_rows.empty:\n",
    "            train_titles.append(matching_rows.iloc[0]['Title'])\n",
    "\n",
    "    # news_id에서 각 news_id에 대한 유사한 publish 시간 뉴스 기사 찾기\n",
    "    negative_ids = []\n",
    "    negative_titles = []\n",
    "    used_ids = set(news_ids)  # 중복을 방지하기 위해 사용된 뉴스 ID를 추적\n",
    "\n",
    "    for news_id in news_ids:\n",
    "        idx = history_news_sorted[history_news_sorted['News ID'] == news_id].index[0]\n",
    "        above_idx, below_idx = idx - 1, idx + 1\n",
    "        similar_ids = []\n",
    "        similar_titles = []\n",
    "\n",
    "        # 이미 존재하는 news_id와 중복을 제외하고 위에서 가장 가까운 두 개의 뉴스를 찾기\n",
    "        above_count = 0\n",
    "        while above_idx >= 0 and above_count < 2:\n",
    "            if history_news_sorted.loc[above_idx, 'News ID'] not in used_ids:\n",
    "                similar_ids.append(history_news_sorted.loc[above_idx, 'News ID'])\n",
    "                similar_titles.append(history_news_sorted.loc[above_idx, 'Title'])\n",
    "                used_ids.add(history_news_sorted.loc[above_idx, 'News ID'])\n",
    "                above_count += 1\n",
    "            above_idx -= 1\n",
    "\n",
    "        # 이미 존재하는 news_id와 중복을 제외하고 아래에서 가장 가까운 두 개의 뉴스를 찾기\n",
    "        below_count = 0\n",
    "        while below_idx < len(history_news_sorted) and below_count < 2:\n",
    "            if history_news_sorted.loc[below_idx, 'News ID'] not in used_ids:\n",
    "                similar_ids.append(history_news_sorted.loc[below_idx, 'News ID'])\n",
    "                similar_titles.append(history_news_sorted.loc[below_idx, 'Title'])\n",
    "                used_ids.add(history_news_sorted.loc[below_idx, 'News ID'])\n",
    "                below_count += 1\n",
    "            below_idx += 1\n",
    "\n",
    "        negative_ids.append(similar_ids)\n",
    "        negative_titles.append(similar_titles)\n",
    "\n",
    "    # Train 데이터에 대한 유사한 뉴스 기사 찾기\n",
    "    negative_train_ids = []\n",
    "    negative_train_titles = []\n",
    "    used_ids = set(train_ids)  # 중복을 방지하기 위해 used_ids에 train_id를 추가\n",
    "\n",
    "    for train_id in train_ids:\n",
    "        idx = train_news_sorted[train_news_sorted['News ID'] == train_id].index[0]\n",
    "        above_idx, below_idx = idx - 1, idx + 1\n",
    "        similar_ids = []\n",
    "        similar_titles = []\n",
    "\n",
    "        # 이미 존재하는 News ID와 중복된 News를 제외하고 위에서 가장 가까운 두 개의 News를 찾기\n",
    "        while above_idx >= 0 and len(similar_ids) < 2:\n",
    "            if train_news_sorted.loc[above_idx, 'News ID'] not in used_ids:\n",
    "                similar_ids.append(train_news_sorted.loc[above_idx, 'News ID'])\n",
    "                similar_titles.append(train_news_sorted.loc[above_idx, 'Title'])\n",
    "                used_ids.add(train_news_sorted.loc[above_idx, 'News ID'])\n",
    "            above_idx -= 1\n",
    "\n",
    "        # 이미 존재하는 News ID와 중복된 News를 제외하고 아래에서 가장 가까운 두 개의 News를 찾기\n",
    "        while below_idx < len(train_news_sorted) and len(similar_ids) < 4:\n",
    "            if train_news_sorted.loc[below_idx, 'News ID'] not in used_ids:\n",
    "                similar_ids.append(train_news_sorted.loc[below_idx, 'News ID'])\n",
    "                similar_titles.append(train_news_sorted.loc[below_idx, 'Title'])\n",
    "                used_ids.add(train_news_sorted.loc[below_idx, 'News ID'])\n",
    "            below_idx += 1\n",
    "\n",
    "        negative_train_ids.append(similar_ids)\n",
    "        negative_train_titles.append(similar_titles)\n",
    "\n",
    "    question_ids = [negative_list + [title] for negative_list, title in zip(negative_train_ids, train_ids)]\n",
    "    question_titles = [negative_list + [title] for negative_list, title in zip(negative_train_titles, train_titles)]\n",
    "\n",
    "    hidden_positions = []\n",
    "\n",
    "    # 각 행을 섞고 히든 값의 위치를 저장\n",
    "    for row in question_titles:\n",
    "        hidden_value = row[-1]  # 히든 값 (마지막 요소)\n",
    "        random.shuffle(row)  # 전체 행을 섞기\n",
    "        hidden_index = row.index(hidden_value)  # 히든 값의 새로운 위치 찾기\n",
    "        hidden_positions.append(hidden_index + 1)  # 히든 값의 위치 저장\n",
    "\n",
    "    hidden_positions_data.append((ID, hidden_positions))\n",
    "    number = len(titles)\n",
    "\n",
    "    # 결과 저장\n",
    "    user_content = \"[News of interest to users]\\n\"\n",
    "    for i in range(number):\n",
    "        combined_titles = [titles[i]] + negative_titles[i]\n",
    "        random.shuffle(combined_titles)\n",
    "        user_content += f\"{i + 1}) \" + \" / \".join(combined_titles) + \"\\n\"\n",
    "        user_content += f\"Of the five news above, the news that the user is most interested in : {titles[i]}\\n\\n\"\n",
    "\n",
    "    user_content += \"[Questions]\\nRank the five candidates for each question based on the user's news interests.\\n\"\n",
    "    for i, titles in enumerate(question_titles):\n",
    "        user_content += f\"Question {i + 1}) \"\n",
    "        user_content += \" / \".join([f\"{j + 1}: {title}\" for j, title in enumerate(titles)]) + \"\\n\"\n",
    "\n",
    "    user_content += \"\\nDo not explain reasons in the response, just return a list of numbers for each article.\\n\"\n",
    "\n",
    "    user_data[ID] = user_content\n",
    "    user_metadata.append((ID, number, len(question_titles)))\n",
    "\n",
    "# 파일 경로 지정 (input 파일이 저장될 폴더 경로)\n",
    "output_folder_path = \"user_prompts/with_negative\"\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# 각 사용자별로 텍스트 파일 생성\n",
    "for user_id, content in user_data.items():\n",
    "    file_path = os.path.join(output_folder_path, f\"{user_id}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "# 각 사용자별 토큰 수와 output 값을 저장할 리스트\n",
    "token_counts_and_outputs = []\n",
    "\n",
    "# tiktoken을 사용하여 토큰 수 계산\n",
    "encoding = tiktoken.encoding_for_model(gpt_name)\n",
    "\n",
    "# 모든 텍스트 파일에 대해 반복 수행\n",
    "for file_path in sorted(glob.glob(os.path.join(output_folder_path, \"*.txt\"))):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # 파일에서 input 텍스트 추출\n",
    "        input_text = file.read()\n",
    "\n",
    "        # 토큰 수 계산\n",
    "        token_count = len(encoding.encode(input_text))\n",
    "\n",
    "        # 사용자 ID 추출\n",
    "        user_id = os.path.basename(file_path).split(\".\")[0]\n",
    "\n",
    "        # Question 수에 따른 output count 계산\n",
    "        num_questions = input_text.count(\"Question \")\n",
    "        output_count = 18 + (num_questions - 1) * 19 if num_questions > 0 else 0\n",
    "\n",
    "        # 사용자 메타데이터에서 number와 num_questions 가져오기\n",
    "        number = next(item[1] for item in user_metadata if item[0] == user_id)\n",
    "\n",
    "        # 사용자 ID, 토큰 수, output count, news_ids 수, question 수를 리스트에 추가\n",
    "        token_counts_and_outputs.append((user_id, token_count, output_count, number, num_questions))\n",
    "\n",
    "# # 결과 출력\n",
    "# for user_id, token_count, output_count, num_news_ids, num_questions in sorted(token_counts_and_outputs, key=lambda x: int(x[0][1:])):\n",
    "#     print(f\"User ID: {user_id:<5} Token Count: {token_count:<6} Output Count: {output_count:<4} History 수: {num_news_ids:<3} Question 수: {num_questions}\")\n",
    "\n",
    "\n",
    "# 결과 출력 (간격 맞추기)\n",
    "meta_folder_path = \"user_prompts/with_negative/metadata\"\n",
    "os.makedirs(meta_folder_path, exist_ok=True)\n",
    "meta_file_path = os.path.join(meta_folder_path, \"output_metadata.txt\")\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_count = 0\n",
    "\n",
    "with open(meta_file_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "    for user_id, token_count, output_count, num_news_ids, num_questions in sorted(token_counts_and_outputs, key=lambda x: int(x[0][1:])):\n",
    "        output_line = (f\"User ID: {user_id:<5} Input Tokens: {token_count:<6} Output Tokens: {output_count:<4}  \"\n",
    "                       f\"History 수: {num_news_ids:<3}  Question 수: {num_questions}\")\n",
    "        total_input_tokens += token_count\n",
    "        total_output_count += output_count\n",
    "\n",
    "        # print(output_line)\n",
    "        meta_file.write(output_line + \"\\n\")\n",
    "\n",
    "    # 전체 토큰 수와 출력 결과 저장\n",
    "    total_line = f\"\\nTotal Input Tokens: {total_input_tokens}\\nTotal Output Tokens: {total_output_count}\"\n",
    "    print(total_line)\n",
    "    meta_file.write(total_line + \"\\n\")\n",
    "\n",
    "\n",
    "# hidden_positions 저장\n",
    "hidden_positions_file_path = os.path.join(meta_folder_path, \"hidden_positions.txt\")\n",
    "with open(hidden_positions_file_path, \"w\", encoding=\"utf-8\") as hidden_file:\n",
    "    for user_id, positions in hidden_positions_data:\n",
    "        hidden_file.write(f\"{user_id:<5}: {positions}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token수만 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: U1    Input Tokens: 2288   Output Tokens: 75    History 수: 18   Question 수: 4\n",
      "User ID: U2    Input Tokens: 2410   Output Tokens: 132   History 수: 16   Question 수: 7\n",
      "User ID: U3    Input Tokens: 2115   Output Tokens: 132   History 수: 14   Question 수: 7\n",
      "User ID: U4    Input Tokens: 1839   Output Tokens: 75    History 수: 14   Question 수: 4\n",
      "User ID: U5    Input Tokens: 3134   Output Tokens: 265   History 수: 17   Question 수: 14\n",
      "User ID: U6    Input Tokens: 3703   Output Tokens: 379   History 수: 17   Question 수: 20\n",
      "User ID: U7    Input Tokens: 5801   Output Tokens: 474   History 수: 34   Question 수: 25\n",
      "User ID: U8    Input Tokens: 1877   Output Tokens: 189   History 수: 9    Question 수: 10\n",
      "User ID: U9    Input Tokens: 2409   Output Tokens: 75    History 수: 19   Question 수: 4\n",
      "User ID: U10   Input Tokens: 2497   Output Tokens: 189   History 수: 15   Question 수: 10\n",
      "User ID: U11   Input Tokens: 4604   Output Tokens: 379   History 수: 27   Question 수: 20\n",
      "User ID: U12   Input Tokens: 3360   Output Tokens: 322   History 수: 16   Question 수: 17\n",
      "User ID: U13   Input Tokens: 2704   Output Tokens: 284   History 수: 12   Question 수: 15\n",
      "User ID: U14   Input Tokens: 2837   Output Tokens: 170   History 수: 18   Question 수: 9\n",
      "User ID: U15   Input Tokens: 1660   Output Tokens: 132   History 수: 10   Question 수: 7\n",
      "User ID: U16   Input Tokens: 3024   Output Tokens: 132   History 수: 23   Question 수: 7\n",
      "User ID: U17   Input Tokens: 1878   Output Tokens: 303   History 수: 4    Question 수: 16\n",
      "User ID: U18   Input Tokens: 4321   Output Tokens: 436   History 수: 21   Question 수: 23\n",
      "User ID: U19   Input Tokens: 3209   Output Tokens: 303   History 수: 16   Question 수: 16\n",
      "User ID: U20   Input Tokens: 1725   Output Tokens: 151   History 수: 9    Question 수: 8\n",
      "User ID: U21   Input Tokens: 3569   Output Tokens: 303   History 수: 20   Question 수: 16\n",
      "User ID: U22   Input Tokens: 5803   Output Tokens: 474   History 수: 33   Question 수: 25\n",
      "User ID: U23   Input Tokens: 3610   Output Tokens: 436   History 수: 14   Question 수: 23\n",
      "User ID: U24   Input Tokens: 3282   Output Tokens: 341   History 수: 16   Question 수: 18\n",
      "User ID: U25   Input Tokens: 2359   Output Tokens: 246   History 수: 11   Question 수: 13\n",
      "User ID: U26   Input Tokens: 4437   Output Tokens: 227   History 수: 32   Question 수: 12\n",
      "User ID: U27   Input Tokens: 1768   Output Tokens: 113   History 수: 11   Question 수: 6\n",
      "User ID: U28   Input Tokens: 4292   Output Tokens: 379   History 수: 23   Question 수: 20\n",
      "User ID: U29   Input Tokens: 1963   Output Tokens: 170   History 수: 10   Question 수: 9\n",
      "User ID: U30   Input Tokens: 1039   Output Tokens: 18    History 수: 9    Question 수: 1\n",
      "User ID: U31   Input Tokens: 3442   Output Tokens: 341   History 수: 17   Question 수: 18\n",
      "User ID: U32   Input Tokens: 1900   Output Tokens: 94    History 수: 15   Question 수: 5\n",
      "User ID: U33   Input Tokens: 3420   Output Tokens: 303   History 수: 18   Question 수: 16\n",
      "User ID: U34   Input Tokens: 2710   Output Tokens: 284   History 수: 12   Question 수: 15\n",
      "User ID: U35   Input Tokens: 2010   Output Tokens: 379   History 수: 2    Question 수: 20\n",
      "User ID: U36   Input Tokens: 2897   Output Tokens: 246   History 수: 16   Question 수: 13\n",
      "User ID: U37   Input Tokens: 3969   Output Tokens: 360   History 수: 21   Question 수: 19\n",
      "User ID: U38   Input Tokens: 4500   Output Tokens: 322   History 수: 28   Question 수: 17\n",
      "User ID: U39   Input Tokens: 2010   Output Tokens: 94    History 수: 14   Question 수: 5\n",
      "User ID: U40   Input Tokens: 3129   Output Tokens: 360   History 수: 13   Question 수: 19\n",
      "User ID: U41   Input Tokens: 3313   Output Tokens: 322   History 수: 17   Question 수: 17\n",
      "User ID: U42   Input Tokens: 1677   Output Tokens: 170   History 수: 8    Question 수: 9\n",
      "User ID: U43   Input Tokens: 2585   Output Tokens: 246   History 수: 13   Question 수: 13\n",
      "User ID: U44   Input Tokens: 2028   Output Tokens: 189   History 수: 10   Question 수: 10\n",
      "User ID: U45   Input Tokens: 828    Output Tokens: 18    History 수: 7    Question 수: 1\n",
      "User ID: U46   Input Tokens: 1396   Output Tokens: 37    History 수: 11   Question 수: 2\n",
      "User ID: U47   Input Tokens: 3171   Output Tokens: 284   History 수: 17   Question 수: 15\n",
      "User ID: U48   Input Tokens: 1954   Output Tokens: 151   History 수: 11   Question 수: 8\n",
      "User ID: U49   Input Tokens: 2128   Output Tokens: 170   History 수: 12   Question 수: 9\n",
      "User ID: U50   Input Tokens: 1807   Output Tokens: 151   History 수: 10   Question 수: 8\n",
      "\n",
      "Input Tokens 합: 140391\n",
      "Output Tokens 합: 11825\n"
     ]
    }
   ],
   "source": [
    "def process_user_data(filename, start_user, end_user):\n",
    "    input_tokens_sum = 0\n",
    "    output_tokens_sum = 0\n",
    "    user_data = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        start_found = False\n",
    "        \n",
    "        for line in lines:\n",
    "            if start_user in line:\n",
    "                start_found = True\n",
    "            \n",
    "            if start_found:\n",
    "                user_data.append(line.strip())\n",
    "                parts = line.split()\n",
    "                input_tokens = int(parts[5])\n",
    "                output_tokens = int(parts[8])\n",
    "                input_tokens_sum += input_tokens\n",
    "                output_tokens_sum += output_tokens\n",
    "                \n",
    "            if end_user in line:\n",
    "                break\n",
    "\n",
    "    # 결과 출력\n",
    "    for data in user_data:\n",
    "        print(data)\n",
    "    print(f\"\\nInput Tokens 합: {input_tokens_sum}\")\n",
    "    print(f\"Output Tokens 합: {output_tokens_sum}\")\n",
    "\n",
    "# 파일 경로 및 USER 범위 지정\n",
    "filename = 'user_prompts/with_negative/metadata/output_metadata.txt'\n",
    "start_user = 'U1'\n",
    "end_user = 'U50'\n",
    "process_user_data(filename, start_user, end_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coongya11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
