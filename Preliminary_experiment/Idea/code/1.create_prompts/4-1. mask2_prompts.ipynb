{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# user DF에서 History column과 Train column을 합쳐서 history로 사용\n",
    "user = pd.read_csv('../../data/user.tsv', sep='\\t', names=['User', 'History', 'Train', 'Question'])\n",
    "user['History'] = user['History'] + user['Train']\n",
    "user = user.drop(columns=['Train'])\n",
    "\n",
    "history_news = pd.read_csv('../../data/history/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "train_news = pd.read_csv('../../data/train/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "history_news = pd.concat([history_news, train_news], ignore_index=True)\n",
    "history_news[['Category_New', 'SubCategory']] = history_news['Category'].str.split('|', expand=True)\n",
    "history_news = history_news.drop('Category', axis=1).rename(columns={'Category_New': 'Category'})\n",
    "\n",
    "question_news = pd.read_csv('../../data/test/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "question_news[['Category_New', 'SubCategory']] = question_news['Category'].str.split('|', expand=True)\n",
    "question_news = question_news.drop('Category', axis=1).rename(columns={'Category_New': 'Category'})\n",
    "\n",
    "\n",
    "# publish 순서에 맞게 오름차순으로 정렬\n",
    "history_news_sorted = history_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)\n",
    "question_news_sorted = question_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>History</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U1</td>\n",
       "      <td>N1,2016-12-31 17:13:57,2017-01-01 08:00:02;N1,...</td>\n",
       "      <td>N4296,2017-01-07 13:08:20,2017-01-08 01:35:11;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2</td>\n",
       "      <td>N1,2016-12-31 17:13:57,2017-01-01 08:00:09;N1,...</td>\n",
       "      <td>N3589,2017-01-06 07:00:00,2017-01-07 18:06:12;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U3</td>\n",
       "      <td>N1,2016-12-31 17:13:57,2017-01-01 08:00:09;N1,...</td>\n",
       "      <td>N4296,2017-01-07 13:08:20,2017-01-08 00:53:08;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U4</td>\n",
       "      <td>N2,2016-12-31 18:06:21,2017-01-01 08:00:15;N2,...</td>\n",
       "      <td>N4475,2017-01-07 18:55:37,2017-01-08 04:28:54;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U5</td>\n",
       "      <td>N3,2016-12-31 15:48:48,2017-01-01 08:00:36;N3,...</td>\n",
       "      <td>N3539,2017-01-06 22:06:33,2017-01-07 12:08:25;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21934</th>\n",
       "      <td>U21935</td>\n",
       "      <td>N1412,2017-01-03 18:24:38,2017-01-04 07:39:56;...</td>\n",
       "      <td>N3890,2017-01-07 07:00:52,2017-01-08 07:43:48;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21935</th>\n",
       "      <td>U21936</td>\n",
       "      <td>N1448,2017-01-03 20:12:15,2017-01-04 07:40:22;...</td>\n",
       "      <td>N4274,2017-01-07 13:51:45,2017-01-08 00:22:07;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21936</th>\n",
       "      <td>U21937</td>\n",
       "      <td>N1379,2017-01-03 16:23:27,2017-01-04 07:44:48;...</td>\n",
       "      <td>N4125,2017-01-07 10:36:22,2017-01-07 20:36:47;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21937</th>\n",
       "      <td>U21938</td>\n",
       "      <td>N1504,2017-01-03 21:56:31,2017-01-04 07:50:25;...</td>\n",
       "      <td>N3556,2017-01-06 18:49:28,2017-01-07 09:45:59;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21938</th>\n",
       "      <td>U21939</td>\n",
       "      <td>N1471,2017-01-03 20:30:00,2017-01-04 07:51:47;...</td>\n",
       "      <td>N3862,2017-01-07 06:23:25,2017-01-07 18:37:30;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21939 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User                                            History  \\\n",
       "0          U1  N1,2016-12-31 17:13:57,2017-01-01 08:00:02;N1,...   \n",
       "1          U2  N1,2016-12-31 17:13:57,2017-01-01 08:00:09;N1,...   \n",
       "2          U3  N1,2016-12-31 17:13:57,2017-01-01 08:00:09;N1,...   \n",
       "3          U4  N2,2016-12-31 18:06:21,2017-01-01 08:00:15;N2,...   \n",
       "4          U5  N3,2016-12-31 15:48:48,2017-01-01 08:00:36;N3,...   \n",
       "...       ...                                                ...   \n",
       "21934  U21935  N1412,2017-01-03 18:24:38,2017-01-04 07:39:56;...   \n",
       "21935  U21936  N1448,2017-01-03 20:12:15,2017-01-04 07:40:22;...   \n",
       "21936  U21937  N1379,2017-01-03 16:23:27,2017-01-04 07:44:48;...   \n",
       "21937  U21938  N1504,2017-01-03 21:56:31,2017-01-04 07:50:25;...   \n",
       "21938  U21939  N1471,2017-01-03 20:30:00,2017-01-04 07:51:47;...   \n",
       "\n",
       "                                                Question  \n",
       "0         N4296,2017-01-07 13:08:20,2017-01-08 01:35:11;  \n",
       "1      N3589,2017-01-06 07:00:00,2017-01-07 18:06:12;...  \n",
       "2         N4296,2017-01-07 13:08:20,2017-01-08 00:53:08;  \n",
       "3         N4475,2017-01-07 18:55:37,2017-01-08 04:28:54;  \n",
       "4      N3539,2017-01-06 22:06:33,2017-01-07 12:08:25;...  \n",
       "...                                                  ...  \n",
       "21934  N3890,2017-01-07 07:00:52,2017-01-08 07:43:48;...  \n",
       "21935  N4274,2017-01-07 13:51:45,2017-01-08 00:22:07;...  \n",
       "21936  N4125,2017-01-07 10:36:22,2017-01-07 20:36:47;...  \n",
       "21937  N3556,2017-01-06 18:49:28,2017-01-07 09:45:59;...  \n",
       "21938  N3862,2017-01-07 06:23:25,2017-01-07 18:37:30;...  \n",
       "\n",
       "[21939 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_ids_and_click_times(news_str):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 뉴스 click history 문자열에서 뉴스 ID와 클릭 시간을 추출하여 각각의 리스트로 반환 (중복된 뉴스 ID는 제외)\n",
    "\n",
    "    설명:\n",
    "    - <news_str>을 세미콜론으로 분리한 후, 각 항목에서 뉴스 ID와 클릭 시간을 추출\n",
    "    - 중복된 뉴스 ID는 <seen_ids> 집합을 사용하여 제외\n",
    "    - 클릭 시간은 pandas의 <to_datetime> 함수를 사용하여 datetime 객체로 변환됨\n",
    "    \"\"\"\n",
    "    news_ids = []     # 뉴스 ID를 저장할 리스트\n",
    "    click_times = []  # 클릭 시간을 저장할 리스트\n",
    "    seen_ids = set()  # 중복된 뉴스 ID를 추적하기 위한 집합\n",
    "    entries = news_str.strip(';').split(';')  # 세미콜론으로 구분된 항목들 분리\n",
    "    for entry in entries:\n",
    "        if entry:\n",
    "            parts = entry.split(',')  # 쉼표로 구분된 부분들 분리\n",
    "            if len(parts) >= 3:\n",
    "                news_id = parts[0]  # 첫 번째 부분을 뉴스 ID로 사용\n",
    "                click_time_str = parts[-1]  # 마지막 부분을 클릭 시간으로 사용\n",
    "                click_time = pd.to_datetime(click_time_str)  # 문자열을 datetime으로 변환\n",
    "                if news_id not in seen_ids:\n",
    "                    seen_ids.add(news_id)  # 중복 방지를 위해 집합에 추가\n",
    "                    news_ids.append(news_id)\n",
    "                    click_times.append(click_time)\n",
    "    return news_ids, click_times\n",
    "\n",
    "def find_similar_news(news_df, news_ids, click_times, used_ids, used_titles, low_freq_categories):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문 뉴스와 유사한 뉴스 기사들 중 'low_freq_categories'에 해당하는 부정적 샘플을 찾아 반환\n",
    "    - 각 질문 뉴스에 대해 최대 4개의 부정적 샘플을 선택\n",
    "\n",
    "    변경 사항:\n",
    "    - 사용자 history category 빈도를 기준으로 low_freq_categories를 필터링 조건으로 추가\n",
    "    - 24시간 이내 뉴스 -> 조건 충족 안되면 더 과거의 뉴스\n",
    "    \"\"\"\n",
    "    negative_ids = []\n",
    "    negative_titles = []\n",
    "\n",
    "    for news_id, click_time in zip(news_ids, click_times):\n",
    "        time_window_start = click_time - pd.Timedelta(hours=24)\n",
    "\n",
    "        # 1. 24시간 이내의 negative sample 후보 찾기\n",
    "        candidate_news = news_df[\n",
    "            (news_df['Publish'] >= time_window_start) & \n",
    "            (news_df['Publish'] <= click_time) & \n",
    "            (~news_df['News ID'].isin(used_ids)) & \n",
    "            (~news_df['Title'].isin(used_titles)) & \n",
    "            (news_df['SubCategory'].isin(low_freq_categories))\n",
    "        ]\n",
    "\n",
    "        # 제목 중복 제거\n",
    "        candidate_news = candidate_news.drop_duplicates(subset='Title')\n",
    "\n",
    "        needed = 4\n",
    "        selected_news_list = []\n",
    "\n",
    "        if len(candidate_news) >= needed:\n",
    "            # 24시간 이내에서 충분히 구할 수 있는 경우\n",
    "            selected_news = candidate_news.sample(n=needed, random_state=42)\n",
    "            selected_news_list.append(selected_news)\n",
    "        else:\n",
    "            # 24시간 이내에서 부족한 경우\n",
    "            selected_news_list.append(candidate_news)\n",
    "            needed -= len(candidate_news)\n",
    "\n",
    "            # 2. 더 과거에서 추가 검색 (가장 최신 순)\n",
    "            extra_news = news_df[\n",
    "                (news_df['Publish'] < time_window_start) &\n",
    "                (~news_df['News ID'].isin(used_ids)) &\n",
    "                (~news_df['Title'].isin(used_titles)) &\n",
    "                (news_df['SubCategory'].isin(low_freq_categories))\n",
    "            ]\n",
    "            extra_news = extra_news.drop_duplicates(subset='Title')\n",
    "\n",
    "            # time_window_start와의 시간 차이를 계산하여 가장 가까운 뉴스부터 선택\n",
    "            extra_news = extra_news.assign(TimeDiff=(time_window_start - extra_news['Publish']).abs())\n",
    "            extra_news = extra_news.sort_values(by='TimeDiff')\n",
    "\n",
    "            extra_selected_news = extra_news.head(needed)\n",
    "            selected_news_list.append(extra_selected_news)\n",
    "\n",
    "        # 선택된 뉴스들 합치기\n",
    "        selected_news = pd.concat(selected_news_list)\n",
    "\n",
    "        # 최종적으로 제목 중복 제거 및 섞기\n",
    "        selected_news = selected_news.drop_duplicates(subset='Title').sample(frac=1, random_state=42)\n",
    "\n",
    "        # negative sample을 최대 4개만 사용\n",
    "        selected_news = selected_news.head(4)\n",
    "\n",
    "        similar_ids = selected_news['News ID'].tolist()\n",
    "        similar_titles = selected_news['Title'].tolist()\n",
    "\n",
    "        negative_ids.append(similar_ids)\n",
    "        negative_titles.append(similar_titles)\n",
    "\n",
    "        # 사용한 ID, Title 업데이트\n",
    "        used_ids.update(similar_ids)\n",
    "        used_titles.update(similar_titles)\n",
    "\n",
    "    return negative_ids, negative_titles\n",
    "\n",
    "def find_similar_question_news(news_df, news_ids, click_times, used_ids, used_titles):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문 뉴스와 유사한 뉴스 기사들 중 부정적 샘플을 찾아 반환\n",
    "    - 각 질문 뉴스에 대해 최대 4개의 부정적 샘플을 선택\n",
    "\n",
    "    설명:\n",
    "    - 각 질문 뉴스에 대해 클릭 시간 기준으로 24시간 이내에 발행된 뉴스 중에서 사용되지 않은 뉴스 ID와 제목을 가진 후보들을 찾음\n",
    "    - 후보들 중에서 중복된 제목을 제거한 후, 필요한 수만큼 랜덤하게 샘플링\n",
    "    - 필요한 수만큼의 샘플이 부족할 경우, 더 과거의 뉴스에서 추가로 샘플을 선택\n",
    "    - 선택된 뉴스 ID와 제목을 리스트에 저장하고, 이후 중복 방지를 위해 used_ids와 used_titles를 업데이트\n",
    "    \"\"\"\n",
    "    negative_ids = []  # negative sample의 뉴스 ID 리스트\n",
    "    negative_titles = []  # negative sample의 뉴스 제목 리스트\n",
    "\n",
    "    for id, click_time in zip(news_ids, click_times):\n",
    "        time_window_start = click_time - pd.Timedelta(hours=24)\n",
    "        candidate_news = news_df[\n",
    "            (news_df['Publish'] >= time_window_start) & \n",
    "            (news_df['Publish'] <= click_time) & \n",
    "            (~news_df['News ID'].isin(used_ids)) & \n",
    "            (~news_df['Title'].isin(used_titles))\n",
    "        ]\n",
    "\n",
    "        # 제목별로 중복 제거\n",
    "        candidate_news = candidate_news.drop_duplicates(subset='Title')\n",
    "\n",
    "        # 필요한 negative sample 수\n",
    "        needed = 4\n",
    "        selected_news_list = []\n",
    "\n",
    "        if len(candidate_news) >= needed:\n",
    "            selected_news = candidate_news.sample(n=needed, random_state=42)  # 랜덤 sampling\n",
    "            selected_news_list.append(selected_news)\n",
    "        else:\n",
    "            selected_news_list.append(candidate_news)\n",
    "            needed -= len(candidate_news)\n",
    "\n",
    "            # 추가로 필요한 negative sample을 더 먼 과거에서 가져오기\n",
    "            extra_news = news_df[\n",
    "                (news_df['Publish'] < time_window_start) &\n",
    "                (~news_df['News ID'].isin(used_ids)) &\n",
    "                (~news_df['Title'].isin(used_titles))\n",
    "            ]\n",
    "            extra_news = extra_news.drop_duplicates(subset='Title')\n",
    "            extra_news = extra_news.assign(TimeDiff=(time_window_start - extra_news['Publish']).abs())\n",
    "            extra_news = extra_news.sort_values(by='TimeDiff')\n",
    "            extra_selected_news = extra_news.head(needed)\n",
    "            selected_news_list.append(extra_selected_news)\n",
    "\n",
    "        # 선택된 뉴스들을 하나의 DataFrame으로 결합\n",
    "        selected_news = pd.concat(selected_news_list)\n",
    "\n",
    "        # 최종적으로 제목 중복 제거 및 섞기\n",
    "        selected_news = selected_news.drop_duplicates(subset='Title')\n",
    "        selected_news = selected_news.sample(frac=1, random_state=42)\n",
    "\n",
    "        # 만약 negative sample 수가 부족하면 가능한 만큼만 사용\n",
    "        selected_news = selected_news.head(4)\n",
    "\n",
    "        similar_ids = selected_news['News ID'].tolist()\n",
    "        similar_titles = selected_news['Title'].tolist()\n",
    "\n",
    "        negative_ids.append(similar_ids)\n",
    "        negative_titles.append(similar_titles)\n",
    "\n",
    "        # 사용된 ID, Title 업데이트\n",
    "        used_ids.update(similar_ids)\n",
    "        used_titles.update(similar_titles)\n",
    "\n",
    "    return negative_ids, negative_titles\n",
    "\n",
    "def save_user_file(user_data, output_folder_path, purpose):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 사용자별 prompt data를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 지정된 output_folder_path 경로에 기존의 .txt 파일들을 삭제\n",
    "    - 각 사용자에 대해 user_id를 파일명으로 하는 .txt 파일을 생성하고, 해당 사용자 데이터를 저장\n",
    "    \"\"\"\n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(output_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(output_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)\n",
    "        print(f'[{purpose}] 기존 User Prompts 삭제')    \n",
    "\n",
    "    os.makedirs(output_folder_path, exist_ok=True)  # 폴더 생성\n",
    "\n",
    "    # user prompt를 각 user별 txt파일로 저장\n",
    "    for user_id, content in user_data.items():\n",
    "        file_path = os.path.join(output_folder_path, f\"{user_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "\n",
    "def save_metadata_file(meta_folder_path, token_counts_and_outputs, user_metadata, purpose):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 메타데이터(토큰 수, 출력 수 등)를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 지정된 meta_folder_path 경로에 기존의 .txt 파일들을 삭제\n",
    "    - 각 사용자에 대해 토큰 수, 출력 수, 히스토리 수, 질문 수 등을 기록하여 output_metadata.txt 파일에 저장\n",
    "    - 총 입력 토큰 수와 출력 토큰 수도 함께 기록\n",
    "    \"\"\"\n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(meta_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(meta_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)    \n",
    "\n",
    "    os.makedirs(meta_folder_path, exist_ok=True)    # 폴더 생성\n",
    "\n",
    "    # metadata 저장 위치\n",
    "    meta_file_path = os.path.join(meta_folder_path, \"output_metadata.txt\")\n",
    "\n",
    "    total_input_tokens = 0  # 총 입력 토큰 수\n",
    "    total_output_count = 0  # 총 출력 토큰 수\n",
    "\n",
    "    # user별 metadata(token 수, history 수, question 수 등) 기록\n",
    "    with open(meta_file_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "        for user_id, token_count, output_count, num_news_ids, num_questions in sorted(token_counts_and_outputs, key=lambda x: int(x[0][1:])):\n",
    "            output_line = (f\"User ID: {user_id:<5} Input Tokens: {token_count:<6} Output Tokens: {output_count:<4}  \"\n",
    "                           f\"History 수: {num_news_ids:<3}  Question 수: {num_questions}\")\n",
    "            total_input_tokens += token_count\n",
    "            total_output_count += output_count\n",
    "\n",
    "            meta_file.write(output_line + \"\\n\")\n",
    "\n",
    "        # 총 합계 기록\n",
    "        total_line = f\"\\nTotal Input Tokens: {total_input_tokens}\\nTotal Output Tokens: {total_output_count}\"\n",
    "        print(f\"[{purpose}] {total_line}\\n\")\n",
    "        meta_file.write(total_line + \"\\n\")\n",
    "\n",
    "def save_hidden_positions(meta_folder_path, hidden_positions_data):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문의 정답 위치(인덱스)를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 각 사용자에 대해 정답 위치를 기록하여 hidden_positions.txt 파일에 저장\n",
    "    \"\"\"\n",
    "    hidden_positions_file_path = os.path.join(meta_folder_path, \"hidden_positions.txt\")\n",
    "    with open(hidden_positions_file_path, \"w\", encoding=\"utf-8\") as hidden_file:\n",
    "        for user_id, positions in hidden_positions_data:\n",
    "            hidden_file.write(f\"{user_id:<5}: {positions}\\n\")\n",
    "\n",
    "def generate_demonstrations(user_name, history_titles, history_negative_titles):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - Demo용 문장을 생성하여 사용자 프롬프트에 추가\n",
    "\n",
    "    설명:\n",
    "    - 사용자의 히스토리 제목과 부정적 샘플 제목을 기반으로 데모 문장을 생성\n",
    "    - 실제 정답의 위치를 [MASK]로 표시하고, 후보 제목들을 무작위로 섞어 표시\n",
    "    \"\"\"\n",
    "    demonstration_lines = []\n",
    "    demonstration_lines.append(f\"[News of Interest to the user]\\n\")\n",
    "\n",
    "    for idx, (h_title, neg_titles) in enumerate(zip(history_titles, history_negative_titles)):\n",
    "        candidates = neg_titles + [h_title]\n",
    "        random.shuffle(candidates)  # 후보 제목들 섞기\n",
    "        index_of_actual = candidates.index(h_title) + 1  # 실제 정답의 인덱스 (1부터 시작)\n",
    "\n",
    "        demonstration_lines.append(f\"{idx+1}) {user_name} prefers most {h_title} among the following five articles:\")\n",
    "        for j, candidate_title in enumerate(candidates):\n",
    "            demonstration_lines.append(f\"{j+1}: {candidate_title}\")\n",
    "        # demonstration_lines.append(f\"The index number of the [MASK] is {index_of_actual}.\\n\")\n",
    "        demonstration_lines.append(f\"\")\n",
    "\n",
    "    return demonstration_lines\n",
    "\n",
    "def create_prompts(purpose, model, user_count, max_question, max_history=300, save_folder=\"user_prompts\", start_count=0):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 전체 프로세스를 통해 사용자별 프롬프트를 생성하고, 관련 파일들을 저장\n",
    "\n",
    "    설명:\n",
    "    - 사용자 데이터를 순회하면서 히스토리 뉴스와 질문 뉴스의 ID 및 클릭 시간을 추출\n",
    "    - positive 및 negative sample을 찾아 프롬프트를 생성\n",
    "    - 생성된 프롬프트와 metadata를 파일로 저장\n",
    "    - purpose에 따라 프롬프트의 내용이 달라지며, with_negative인 경우 부정적 샘플을 포함\n",
    "    \"\"\"\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    formatted_date = now.strftime(\"%y%m%d\")\n",
    "\n",
    "    save_folder = f'../../prompts/[{formatted_date}-4] {save_folder}'\n",
    "\n",
    "    # 사용자별 데이터 초기화\n",
    "    user_data = {}\n",
    "    token_counts_and_outputs = []\n",
    "    hidden_positions_data = []\n",
    "\n",
    "    # 히스토리의 최대 길이 설정\n",
    "    max_history = int(f'-{max_history}')\n",
    "\n",
    "    # tiktoken을 사용하여 토큰 수 계산\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    # 사용자 목록 생성 (데이터프레임 'user'의 상위 user_count만 선택)\n",
    "    selected_users = user.iloc[start_count:user_count]\n",
    "\n",
    "    for idx, row in selected_users.iterrows():\n",
    "        user_id = row['User']\n",
    "        user_name = f\"User #{user_id[1:]}\"  # 사용자 이름 형식 지정\n",
    "\n",
    "        # history 뉴스 ID 및 클릭 시간 추출 (중복 제거됨)\n",
    "        history_str = row['History']\n",
    "        history_news_ids, history_click_times = extract_news_ids_and_click_times(history_str)\n",
    "        history_news_ids = history_news_ids[max_history:]\n",
    "        history_click_times = history_click_times[max_history:]\n",
    "\n",
    "        # history 뉴스 Title 추출\n",
    "        history_titles = []\n",
    "        for news_id in history_news_ids:\n",
    "            matching_rows = history_news[history_news['News ID'] == news_id]\n",
    "            if not matching_rows.empty:\n",
    "                title = matching_rows.iloc[0]['Title']\n",
    "                history_titles.append(title)\n",
    "\n",
    "        # question 뉴스 ID 및 클릭 시간 추출 (중복 제거됨)\n",
    "        question_str = row['Question']\n",
    "        question_news_ids, question_click_times = extract_news_ids_and_click_times(question_str)\n",
    "\n",
    "        # question 뉴스 제목 추출\n",
    "        question_titles_list = []\n",
    "        for news_id in question_news_ids:\n",
    "            matching_rows = question_news[question_news['News ID'] == news_id]\n",
    "            if not matching_rows.empty:\n",
    "                title = matching_rows.iloc[0]['Title']\n",
    "                question_titles_list.append(title)\n",
    "                \n",
    "                \n",
    "\n",
    "        # used_ids와 used_titles 초기화 (이미 사용된 ID와 제목)\n",
    "        used_ids = set(history_news_ids + question_news_ids)\n",
    "        used_titles = set(history_titles + question_titles_list)\n",
    "        \n",
    "\n",
    "        user_history_categories = history_news[history_news['News ID'].isin(history_news_ids)]['SubCategory']\n",
    "        category_counts = Counter(user_history_categories)\n",
    "        all_categories = set(history_news['SubCategory'].unique()).union(question_news['SubCategory'].unique())\n",
    "        low_freq_categories = {cat for cat in all_categories if category_counts[cat] <= 0}\n",
    "\n",
    "        # history 뉴스에 대한 negative 샘플 찾기\n",
    "        if purpose == 'with_negative':\n",
    "            history_negative_ids, history_negative_titles = find_similar_news(\n",
    "                history_news, history_news_ids, history_click_times, used_ids, used_titles, low_freq_categories\n",
    "            )\n",
    "\n",
    "            # 데모 생성\n",
    "            demonstration_lines = generate_demonstrations(\n",
    "                user_name, history_titles, history_negative_titles\n",
    "            )\n",
    "        else:\n",
    "            demonstration_lines = []\n",
    "\n",
    "        # question 뉴스에 대한 negative sample 찾기\n",
    "        negative_question_ids, negative_question_titles = find_similar_question_news(\n",
    "            question_news, question_news_ids, question_click_times, used_ids, used_titles\n",
    "        )\n",
    "\n",
    "        # 각 question에 대한 candidate title 준비\n",
    "        all_candidate_titles = []\n",
    "        hidden_positions = []\n",
    "\n",
    "        for q_title, neg_titles in zip(question_titles_list, negative_question_titles):\n",
    "            \n",
    "            neg_titles = [title for title in neg_titles if title != q_title]    # neg_titles에서 q_title과 동일한 제목을 제거\n",
    "            neg_titles = list(dict.fromkeys(neg_titles))    # neg_titles에서 제목의 중복 제거\n",
    "\n",
    "            # negative 샘플이 4개 미만일 경우 처리  (현재는 반복을 중단)\n",
    "            while len(neg_titles) < 4:\n",
    "                break  # 일단 반복을 중단하고 현재 상태로 진행\n",
    "\n",
    "            # 최대 4개의 negative 샘플과 q_title을 후보로 사용\n",
    "            candidates = neg_titles[:4] + [q_title]\n",
    "            # 후보에서 제목의 중복 제거\n",
    "            candidates = list(dict.fromkeys(candidates))\n",
    "\n",
    "            # 후보가 5개가 되도록 보장 (필요 시 추가 로직 구현 가능)\n",
    "            if len(candidates) < 5:\n",
    "                print(f'idx, question수 5개 미만')\n",
    "                pass\n",
    "\n",
    "            random.shuffle(candidates)  # 후보 섞기\n",
    "            all_candidate_titles.append(candidates)\n",
    "            index_of_actual = candidates.index(q_title) + 1  # 실제 정답의 인덱스 (1부터 시작)\n",
    "            hidden_positions.append(index_of_actual)\n",
    "\n",
    "\n",
    "        # 프롬프트 텍스트 생성\n",
    "        prompt_lines = []\n",
    "\n",
    "        # 데모 부분 추가\n",
    "        prompt_lines.extend(demonstration_lines)\n",
    "\n",
    "        # 사용자의 클릭 히스토리 추가\n",
    "        if purpose == 'only_positive':\n",
    "            prompt_lines.append(f\"[Click History]\\nThe news articles that {user_name} clicked before are as follows:\")\n",
    "            for i, title in enumerate(history_titles):\n",
    "                prompt_lines.append(f\"{i+1}. {title}\")\n",
    "            prompt_lines.append(\"\\n\")\n",
    "\n",
    "        # question 부분 추가\n",
    "        prompt_lines.append(f\"[Questions]\\nBased on {user_name}'s preferences, predict the index number of the news article that best fits the position labeled [MASK] for each question.\\n\")\n",
    "\n",
    "        for i, candidates in enumerate(all_candidate_titles[:max_question]):\n",
    "            # if purpose == 'with_negative':\n",
    "            prompt_lines.append(f\"Question {i+1}) {user_name} prefers most [MASK] among the following five articles:\")\n",
    "            # else:\n",
    "            #     prompt_lines.append(f\"Based on the articles the user clicked before, {user_name} prefers most [MASK] among the following five articles:\")\n",
    "            for j, candidate_title in enumerate(candidates):\n",
    "                prompt_lines.append(f\"{j+1}: {candidate_title}\")\n",
    "            prompt_lines.append(\"\")\n",
    "            # prompt_lines.append(f\"Question {i+1}. The index number of the [MASK] is ?\\n\")\n",
    "            num_questions = i+1\n",
    "\n",
    "        prompt_lines.append(f\"Please provide just the answers to each of {user_name}'s question without any explanations.\")\n",
    "        prompt_text = '\\n'.join(prompt_lines)\n",
    "        \n",
    "        user_data[user_id] = prompt_text\n",
    "        hidden_positions_data.append((user_id, hidden_positions))\n",
    "        token_count = len(encoding.encode(prompt_text))  # 토큰 수 계산\n",
    "        output_count = num_questions  # 출력 수\n",
    "        num_history_titles = len(history_titles)  # 히스토리 제목 수\n",
    "        token_counts_and_outputs.append((user_id, token_count, output_count, num_history_titles, num_questions))\n",
    "\n",
    "    # prompt 및 metadata 저장\n",
    "    output_folder_path = os.path.join(save_folder, purpose)\n",
    "    save_user_file(user_data, output_folder_path, purpose)\n",
    "\n",
    "    # metadata 파일 저장\n",
    "    meta_folder_path = os.path.join(save_folder, purpose, 'metadata')\n",
    "    save_metadata_file(meta_folder_path, token_counts_and_outputs, None, purpose)\n",
    "    save_hidden_positions(meta_folder_path, hidden_positions_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행\n",
    "[create 함수]\n",
    "- purpose = 어떤 목적으로 prompt를 생성할 것인지  [only_positive / with_negative]   \n",
    "- model = 사용할 gpt (token 계산 용도)  [gpt-4o-mini / gpt-3.5-turbo]\n",
    "- user_count = 몇 명의 user prompt를 생성할 것인지\n",
    "- max_question = 최대 질문 수\n",
    "- save_forder = 결과를 저장할 폴더 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[only_positive] 기존 User Prompts 삭제\n",
      "[only_positive] \n",
      "Total Input Tokens: 2926099\n",
      "Total Output Tokens: 13184\n",
      "\n",
      "[with_negative] 기존 User Prompts 삭제\n",
      "[with_negative] \n",
      "Total Input Tokens: 10670393\n",
      "Total Output Tokens: 13184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_prompts(purpose='only_positive',\n",
    "               model=\"gpt-4o-mini\",\n",
    "               user_count=3000,\n",
    "               max_question=15,\n",
    "               max_history=60,\n",
    "               save_folder=\"1~3000\",\n",
    "               start_count=0\n",
    "               )\n",
    "\n",
    "create_prompts(purpose='with_negative',\n",
    "               model=\"gpt-4o-mini\",\n",
    "               user_count=3000,\n",
    "               max_question=15,\n",
    "               max_history=60,\n",
    "               save_folder=\"1~3000\",\n",
    "               start_count=0\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[only_positive] \n",
      "Total Input Tokens: 9547652\n",
      "Total Output Tokens: 42563\n",
      "\n",
      "[with_negative] \n",
      "Total Input Tokens: 34801469\n",
      "Total Output Tokens: 42563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_prompts(purpose='only_positive',\n",
    "               model=\"gpt-4o-mini\",\n",
    "               user_count=10000,\n",
    "               max_question=15,\n",
    "               max_history=60,\n",
    "               save_folder=\"1~10000\",\n",
    "               start_count=0\n",
    "               )\n",
    "\n",
    "create_prompts(purpose='with_negative',\n",
    "               model=\"gpt-4o-mini\",\n",
    "               user_count=10000,\n",
    "               max_question=15,\n",
    "               max_history=60,\n",
    "               save_folder=\"1~10000\",\n",
    "               start_count=0\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
