{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# user DF에서 History column과 Train column을 합쳐서 history로 사용\n",
    "user = pd.read_csv('../../data/user.tsv', sep='\\t', names=['User', 'History', 'Train', 'Question'])\n",
    "user['History'] = user['History'] + user['Train']\n",
    "user = user.drop(columns=['Train'])\n",
    "\n",
    "history_news = pd.read_csv('../../data/history/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "train_news = pd.read_csv('../../data/train/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "history_news = pd.concat([history_news, train_news], ignore_index=True)\n",
    "history_news[['Category_New', 'SubCategory']] = history_news['Category'].str.split('|', expand=True)\n",
    "history_news = history_news.drop('Category', axis=1).rename(columns={'Category_New': 'Category'})\n",
    "\n",
    "question_news = pd.read_csv('../../data/test/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "question_news[['Category_New', 'SubCategory']] = question_news['Category'].str.split('|', expand=True)\n",
    "question_news = question_news.drop('Category', axis=1).rename(columns={'Category_New': 'Category'})\n",
    "\n",
    "\n",
    "# publish 순서에 맞게 오름차순으로 정렬\n",
    "history_news_sorted = history_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)\n",
    "question_news_sorted = question_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_ids_and_click_times(news_str):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 뉴스 click history 문자열에서 뉴스 ID와 클릭 시간을 추출하여 각각의 리스트로 반환 (중복된 뉴스 ID는 제외)\n",
    "\n",
    "    설명:\n",
    "    - <news_str>을 세미콜론으로 분리한 후, 각 항목에서 뉴스 ID와 클릭 시간을 추출\n",
    "    - 중복된 뉴스 ID는 <seen_ids> 집합을 사용하여 제외\n",
    "    - 클릭 시간은 pandas의 <to_datetime> 함수를 사용하여 datetime 객체로 변환됨\n",
    "    \"\"\"\n",
    "    news_ids = []     # 뉴스 ID를 저장할 리스트\n",
    "    click_times = []  # 클릭 시간을 저장할 리스트\n",
    "    seen_ids = set()  # 중복된 뉴스 ID를 추적하기 위한 집합\n",
    "    entries = news_str.strip(';').split(';')  # 세미콜론으로 구분된 항목들 분리\n",
    "    for entry in entries:\n",
    "        if entry:\n",
    "            parts = entry.split(',')  # 쉼표로 구분된 부분들 분리\n",
    "            if len(parts) >= 3:\n",
    "                news_id = parts[0]  # 첫 번째 부분을 뉴스 ID로 사용\n",
    "                click_time_str = parts[-1]  # 마지막 부분을 클릭 시간으로 사용\n",
    "                click_time = pd.to_datetime(click_time_str)  # 문자열을 datetime으로 변환\n",
    "                if news_id not in seen_ids:\n",
    "                    seen_ids.add(news_id)  # 중복 방지를 위해 집합에 추가\n",
    "                    news_ids.append(news_id)\n",
    "                    click_times.append(click_time)\n",
    "    return news_ids, click_times\n",
    "\n",
    "def find_similar_news(news_df, news_ids, click_times, used_ids, used_titles, low_freq_categories):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문 뉴스와 유사한 뉴스 기사들 중 'low_freq_categories'에 해당하는 부정적 샘플을 찾아 반환\n",
    "    - 각 질문 뉴스에 대해 최대 4개의 부정적 샘플을 선택\n",
    "\n",
    "    변경 사항:\n",
    "    - 사용자 history category 빈도를 기준으로 low_freq_categories를 필터링 조건으로 추가\n",
    "    - 24시간 이내 뉴스 -> 조건 충족 안되면 더 과거의 뉴스\n",
    "    \"\"\"\n",
    "    negative_ids = []\n",
    "    negative_titles = []\n",
    "\n",
    "    for news_id, click_time in zip(news_ids, click_times):\n",
    "        time_window_start = click_time - pd.Timedelta(hours=24)\n",
    "\n",
    "        # 1. 24시간 이내의 negative sample 후보 찾기\n",
    "        candidate_news = news_df[\n",
    "            (news_df['Publish'] >= time_window_start) & \n",
    "            (news_df['Publish'] <= click_time) & \n",
    "            (~news_df['News ID'].isin(used_ids)) & \n",
    "            (~news_df['Title'].isin(used_titles)) & \n",
    "            (news_df['SubCategory'].isin(low_freq_categories))\n",
    "        ]\n",
    "\n",
    "        # 제목 중복 제거\n",
    "        candidate_news = candidate_news.drop_duplicates(subset='Title')\n",
    "\n",
    "        needed = 4\n",
    "        selected_news_list = []\n",
    "\n",
    "        if len(candidate_news) >= needed:\n",
    "            # 24시간 이내에서 충분히 구할 수 있는 경우\n",
    "            selected_news = candidate_news.sample(n=needed, random_state=42)\n",
    "            selected_news_list.append(selected_news)\n",
    "        else:\n",
    "            # 24시간 이내에서 부족한 경우\n",
    "            selected_news_list.append(candidate_news)\n",
    "            needed -= len(candidate_news)\n",
    "\n",
    "            # 2. 더 과거에서 추가 검색 (가장 최신 순)\n",
    "            extra_news = news_df[\n",
    "                (news_df['Publish'] < time_window_start) &\n",
    "                (~news_df['News ID'].isin(used_ids)) &\n",
    "                (~news_df['Title'].isin(used_titles)) &\n",
    "                (news_df['SubCategory'].isin(low_freq_categories))\n",
    "            ]\n",
    "            extra_news = extra_news.drop_duplicates(subset='Title')\n",
    "\n",
    "            # time_window_start와의 시간 차이를 계산하여 가장 가까운 뉴스부터 선택\n",
    "            extra_news = extra_news.assign(TimeDiff=(time_window_start - extra_news['Publish']).abs())\n",
    "            extra_news = extra_news.sort_values(by='TimeDiff')\n",
    "\n",
    "            extra_selected_news = extra_news.head(needed)\n",
    "            selected_news_list.append(extra_selected_news)\n",
    "\n",
    "        # 선택된 뉴스들 합치기\n",
    "        selected_news = pd.concat(selected_news_list)\n",
    "\n",
    "        # 최종적으로 제목 중복 제거 및 섞기\n",
    "        selected_news = selected_news.drop_duplicates(subset='Title').sample(frac=1, random_state=42)\n",
    "\n",
    "        # negative sample을 최대 4개만 사용\n",
    "        selected_news = selected_news.head(4)\n",
    "\n",
    "        similar_ids = selected_news['News ID'].tolist()\n",
    "        similar_titles = selected_news['Title'].tolist()\n",
    "\n",
    "        negative_ids.append(similar_ids)\n",
    "        negative_titles.append(similar_titles)\n",
    "\n",
    "        # 사용한 ID, Title 업데이트\n",
    "        used_ids.update(similar_ids)\n",
    "        used_titles.update(similar_titles)\n",
    "\n",
    "    return negative_ids, negative_titles\n",
    "\n",
    "def find_similar_question_news(news_df, news_ids, click_times, used_ids, used_titles):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문 뉴스와 유사한 뉴스 기사들 중 부정적 샘플을 찾아 반환\n",
    "    - 각 질문 뉴스에 대해 최대 4개의 부정적 샘플을 선택\n",
    "\n",
    "    설명:\n",
    "    - 각 질문 뉴스에 대해 클릭 시간 기준으로 24시간 이내에 발행된 뉴스 중에서 사용되지 않은 뉴스 ID와 제목을 가진 후보들을 찾음\n",
    "    - 후보들 중에서 중복된 제목을 제거한 후, 필요한 수만큼 랜덤하게 샘플링\n",
    "    - 필요한 수만큼의 샘플이 부족할 경우, 더 과거의 뉴스에서 추가로 샘플을 선택\n",
    "    - 선택된 뉴스 ID와 제목을 리스트에 저장하고, 이후 중복 방지를 위해 used_ids와 used_titles를 업데이트\n",
    "    \"\"\"\n",
    "    negative_ids = []  # negative sample의 뉴스 ID 리스트\n",
    "    negative_titles = []  # negative sample의 뉴스 제목 리스트\n",
    "\n",
    "    for id, click_time in zip(news_ids, click_times):\n",
    "        time_window_start = click_time - pd.Timedelta(hours=24)\n",
    "        candidate_news = news_df[\n",
    "            (news_df['Publish'] >= time_window_start) & \n",
    "            (news_df['Publish'] <= click_time) & \n",
    "            (~news_df['News ID'].isin(used_ids)) & \n",
    "            (~news_df['Title'].isin(used_titles))\n",
    "        ]\n",
    "\n",
    "        # 제목별로 중복 제거\n",
    "        candidate_news = candidate_news.drop_duplicates(subset='Title')\n",
    "\n",
    "        # 필요한 negative sample 수\n",
    "        needed = 4\n",
    "        selected_news_list = []\n",
    "\n",
    "        if len(candidate_news) >= needed:\n",
    "            selected_news = candidate_news.sample(n=needed, random_state=42)  # 랜덤 sampling\n",
    "            selected_news_list.append(selected_news)\n",
    "        else:\n",
    "            selected_news_list.append(candidate_news)\n",
    "            needed -= len(candidate_news)\n",
    "\n",
    "            # 추가로 필요한 negative sample을 더 먼 과거에서 가져오기\n",
    "            extra_news = news_df[\n",
    "                (news_df['Publish'] < time_window_start) &\n",
    "                (~news_df['News ID'].isin(used_ids)) &\n",
    "                (~news_df['Title'].isin(used_titles))\n",
    "            ]\n",
    "            extra_news = extra_news.drop_duplicates(subset='Title')\n",
    "            extra_news = extra_news.assign(TimeDiff=(time_window_start - extra_news['Publish']).abs())\n",
    "            extra_news = extra_news.sort_values(by='TimeDiff')\n",
    "            extra_selected_news = extra_news.head(needed)\n",
    "            selected_news_list.append(extra_selected_news)\n",
    "\n",
    "        # 선택된 뉴스들을 하나의 DataFrame으로 결합\n",
    "        selected_news = pd.concat(selected_news_list)\n",
    "\n",
    "        # 최종적으로 제목 중복 제거 및 섞기\n",
    "        selected_news = selected_news.drop_duplicates(subset='Title')\n",
    "        selected_news = selected_news.sample(frac=1, random_state=42)\n",
    "\n",
    "        # 만약 negative sample 수가 부족하면 가능한 만큼만 사용\n",
    "        selected_news = selected_news.head(4)\n",
    "\n",
    "        similar_ids = selected_news['News ID'].tolist()\n",
    "        similar_titles = selected_news['Title'].tolist()\n",
    "\n",
    "        negative_ids.append(similar_ids)\n",
    "        negative_titles.append(similar_titles)\n",
    "\n",
    "        # 사용된 ID, Title 업데이트\n",
    "        used_ids.update(similar_ids)\n",
    "        used_titles.update(similar_titles)\n",
    "\n",
    "    return negative_ids, negative_titles\n",
    "\n",
    "def save_user_file(user_data, output_folder_path, purpose):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 사용자별 prompt data를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 지정된 output_folder_path 경로에 기존의 .txt 파일들을 삭제\n",
    "    - 각 사용자에 대해 user_id를 파일명으로 하는 .txt 파일을 생성하고, 해당 사용자 데이터를 저장\n",
    "    \"\"\"\n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(output_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(output_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)\n",
    "        print(f'[{purpose}] 기존 User Prompts 삭제')    \n",
    "\n",
    "    os.makedirs(output_folder_path, exist_ok=True)  # 폴더 생성\n",
    "\n",
    "    # user prompt를 각 user별 txt파일로 저장\n",
    "    for user_id, content in user_data.items():\n",
    "        file_path = os.path.join(output_folder_path, f\"{user_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "\n",
    "def save_metadata_file(meta_folder_path, token_counts_and_outputs, user_metadata, purpose):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 메타데이터(토큰 수, 출력 수 등)를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 지정된 meta_folder_path 경로에 기존의 .txt 파일들을 삭제\n",
    "    - 각 사용자에 대해 토큰 수, 출력 수, 히스토리 수, 질문 수 등을 기록하여 output_metadata.txt 파일에 저장\n",
    "    - 총 입력 토큰 수와 출력 토큰 수도 함께 기록\n",
    "    \"\"\"\n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(meta_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(meta_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)    \n",
    "\n",
    "    os.makedirs(meta_folder_path, exist_ok=True)    # 폴더 생성\n",
    "\n",
    "    # metadata 저장 위치\n",
    "    meta_file_path = os.path.join(meta_folder_path, \"output_metadata.txt\")\n",
    "\n",
    "    total_input_tokens = 0  # 총 입력 토큰 수\n",
    "    total_output_count = 0  # 총 출력 토큰 수\n",
    "\n",
    "    # user별 metadata(token 수, history 수, question 수 등) 기록\n",
    "    with open(meta_file_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "        for user_id, token_count, output_count, num_news_ids, num_questions in sorted(token_counts_and_outputs, key=lambda x: int(x[0][1:])):\n",
    "            output_line = (f\"User ID: {user_id:<5} Input Tokens: {token_count:<6} Output Tokens: {output_count:<4}  \"\n",
    "                           f\"History 수: {num_news_ids:<3}  Question 수: {num_questions}\")\n",
    "            total_input_tokens += token_count\n",
    "            total_output_count += output_count\n",
    "\n",
    "            meta_file.write(output_line + \"\\n\")\n",
    "\n",
    "        # 총 합계 기록\n",
    "        total_line = f\"\\nTotal Input Tokens: {total_input_tokens}\\nTotal Output Tokens: {total_output_count}\"\n",
    "        print(f\"[{purpose}] {total_line}\\n\")\n",
    "        meta_file.write(total_line + \"\\n\")\n",
    "\n",
    "def save_hidden_positions(meta_folder_path, hidden_positions_data):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문의 정답 위치(인덱스)를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 각 사용자에 대해 정답 위치를 기록하여 hidden_positions.txt 파일에 저장\n",
    "    \"\"\"\n",
    "    hidden_positions_file_path = os.path.join(meta_folder_path, \"hidden_positions.txt\")\n",
    "    with open(hidden_positions_file_path, \"w\", encoding=\"utf-8\") as hidden_file:\n",
    "        for user_id, positions in hidden_positions_data:\n",
    "            hidden_file.write(f\"{user_id:<5}: {positions}\\n\")\n",
    "\n",
    "def create(purpose, model, user_count, max_question, max_history=300, save_folder=\"user_prompts\", start_count=0):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 전체 프로세스를 통해 사용자별 프롬프트를 생성하고, 관련 파일들을 저장\n",
    "\n",
    "    설명:\n",
    "    - 사용자 데이터를 순회하면서 히스토리 뉴스와 질문 뉴스의 ID 및 클릭 시간을 추출\n",
    "    - positive 및 negative sample을 찾아 프롬프트를 생성\n",
    "    - 생성된 프롬프트와 metadata를 파일로 저장\n",
    "    - purpose에 따라 프롬프트의 내용이 달라지며, with_negative인 경우 부정적 샘플을 포함\n",
    "    \"\"\"\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    formatted_date = now.strftime(\"%y%m%d\")\n",
    "\n",
    "    save_folder = f'../../prompts/[{formatted_date}-1] {save_folder}'\n",
    "    \n",
    "    # 사용자별 데이터 초기화\n",
    "    user_data = {}\n",
    "    token_counts_and_outputs = []\n",
    "    hidden_positions_data = []\n",
    "\n",
    "    # 히스토리의 최대 길이 설정\n",
    "    max_history = int(f'-{max_history}')\n",
    "\n",
    "    # tiktoken을 사용하여 토큰 수 계산\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    # 사용자 목록 생성 (데이터프레임 'user'의 상위 user_count만 선택)\n",
    "    selected_users = user.iloc[start_count:user_count]\n",
    "\n",
    "    for idx, row in selected_users.iterrows():\n",
    "        user_id = row['User']\n",
    "        user_name = f\"User #{user_id[1:]}\"  # 사용자 이름 형식 지정\n",
    "\n",
    "        # history 뉴스 ID 및 클릭 시간 추출 (중복 제거됨)\n",
    "        history_str = row['History']\n",
    "        history_news_ids, history_click_times = extract_news_ids_and_click_times(history_str)\n",
    "        history_news_ids = history_news_ids[max_history:]\n",
    "        history_click_times = history_click_times[max_history:]\n",
    "\n",
    "        # history 뉴스 Title 추출\n",
    "        history_titles = []\n",
    "        for news_id in history_news_ids:\n",
    "            matching_rows = history_news[history_news['News ID'] == news_id]\n",
    "            if not matching_rows.empty:\n",
    "                title = matching_rows.iloc[0]['Title']\n",
    "                history_titles.append(title)\n",
    "\n",
    "        # question 뉴스 ID 및 클릭 시간 추출 (중복 제거됨)\n",
    "        question_str = row['Question']\n",
    "        question_news_ids, question_click_times = extract_news_ids_and_click_times(question_str)\n",
    "\n",
    "        # question 뉴스 제목 추출\n",
    "        question_titles_list = []\n",
    "        for news_id in question_news_ids:\n",
    "            matching_rows = question_news[question_news['News ID'] == news_id]\n",
    "            if not matching_rows.empty:\n",
    "                title = matching_rows.iloc[0]['Title']\n",
    "                question_titles_list.append(title)\n",
    "                \n",
    "                \n",
    "\n",
    "        # used_ids와 used_titles 초기화 (이미 사용된 ID와 제목)\n",
    "        used_ids = set(history_news_ids + question_news_ids)\n",
    "        used_titles = set(history_titles + question_titles_list)\n",
    "        \n",
    "\n",
    "        user_history_categories = history_news[history_news['News ID'].isin(history_news_ids)]['SubCategory']\n",
    "        category_counts = Counter(user_history_categories)\n",
    "        all_categories = set(history_news['SubCategory'].unique()).union(question_news['SubCategory'].unique())\n",
    "        low_freq_categories = {cat for cat in all_categories if category_counts[cat] <= 0}\n",
    "\n",
    "        # history 뉴스에 대한 negative 샘플 찾기 (CODE 2의 방식 사용)\n",
    "        if purpose == \"with_negative\":\n",
    "            history_negative_ids, history_negative_titles = find_similar_news(\n",
    "                history_news, history_news_ids, history_click_times, used_ids, used_titles, low_freq_categories\n",
    "            )\n",
    "\n",
    "        # question 뉴스에 대한 negative 샘플 찾기\n",
    "        negative_question_ids, negative_question_titles = find_similar_question_news(\n",
    "            question_news, question_news_ids, question_click_times, used_ids, used_titles\n",
    "        )\n",
    "\n",
    "            \n",
    "        # 각 질문에 대한 후보 타이틀 준비\n",
    "        all_candidate_titles = []\n",
    "        hidden_positions = []\n",
    "\n",
    "        for q_title, neg_titles in zip(question_titles_list, negative_question_titles):\n",
    "            candidates = neg_titles + [q_title]\n",
    "            random.shuffle(candidates)  # 동일한 random state 사용\n",
    "            all_candidate_titles.append(candidates)\n",
    "            index_of_actual = candidates.index(q_title) + 1  # 번호가 1부터 시작하므로 +1\n",
    "            hidden_positions.append(index_of_actual)\n",
    "\n",
    "        # 정답 위치 데이터 추가\n",
    "        hidden_positions_data.append((user_id, hidden_positions))\n",
    "        num_questions = len(all_candidate_titles)\n",
    "\n",
    "        # 프롬프트 텍스트 생성\n",
    "        prompt_lines = []\n",
    "\n",
    "        if purpose == \"only_positive\":\n",
    "            # Click History 섹션 추가 (CODE 1의 형식 유지)\n",
    "            prompt_lines.append(\"[Click History]\")\n",
    "            for i, title in enumerate(history_titles):\n",
    "                prompt_lines.append(f\"{i+1}) click : {title}\")\n",
    "            prompt_lines.append(\"\\n[Questions]\")\n",
    "            prompt_lines.append(\"Returns the most likely clickable news among the candidate news for each question based on the user's news interests.\")\n",
    "            for i, candidates in enumerate(all_candidate_titles[:max_question]):\n",
    "                candidate_str = \" / \".join([f\"{j+1}: {title}\" for j, title in enumerate(candidates)])\n",
    "                prompt_lines.append(f\"Question {i+1}) {candidate_str}\")\n",
    "            prompt_lines.append(\"\\nDon't explain why in your answer, just return the number of the news.\")\n",
    "\n",
    "        elif purpose == \"with_negative\":\n",
    "            # News of interest to the user 섹션 추가 (CODE 1의 형식 유지)\n",
    "            prompt_lines.append(\"[News of interest to the user]\")\n",
    "            for i, (title, neg_titles) in enumerate(zip(history_titles, history_negative_titles)):\n",
    "                combined_titles = neg_titles + [title]\n",
    "                random.Random(42).shuffle(combined_titles)  # 동일한 random state 사용\n",
    "                prompt_lines.append(f\"{i+1}) \" + \" / \".join(combined_titles))\n",
    "                prompt_lines.append(f\"Of the five news above, the news that the user is most interested in : {title}\\n\")\n",
    "\n",
    "            prompt_lines.append(\"[Questions]\")\n",
    "            prompt_lines.append(\"Returns the most likely clickable news among the candidate news for each question based on the user's news interests.\")\n",
    "            for i, candidates in enumerate(all_candidate_titles[:max_question]):\n",
    "                candidate_str = \" / \".join([f\"{j+1}: {title}\" for j, title in enumerate(candidates)])\n",
    "                prompt_lines.append(f\"Question {i+1}) {candidate_str}\")\n",
    "            prompt_lines.append(\"\\nDon't explain why in your answer, just return the number of the news.\")\n",
    "\n",
    "        prompt_text = '\\n'.join(prompt_lines)\n",
    "        user_data[user_id] = prompt_text\n",
    "        token_count = len(encoding.encode(prompt_text))\n",
    "        output_count = num_questions\n",
    "        num_history_titles = len(history_titles)\n",
    "        token_counts_and_outputs.append((user_id, token_count, output_count, num_history_titles, num_questions))\n",
    "\n",
    "    # 프롬프트 및 메타데이터 저장\n",
    "    output_folder_path = os.path.join(save_folder, purpose)\n",
    "    save_user_file(user_data, output_folder_path, purpose)\n",
    "\n",
    "    # 메타데이터 파일 저장\n",
    "    meta_folder_path = os.path.join(save_folder, purpose, 'metadata')\n",
    "    save_metadata_file(meta_folder_path, token_counts_and_outputs, None, purpose)\n",
    "    save_hidden_positions(meta_folder_path, hidden_positions_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행\n",
    "[create 함수]\n",
    "- purpose = 어떤 목적으로 prompt를 생성할 것인지  [only_positive / with_negative]   \n",
    "- model = 사용할 gpt (token 계산 용도)  [gpt-4o-mini / gpt-3.5-turbo]\n",
    "- user_count = 몇 명의 user prompt를 생성할 것인지\n",
    "- max_question = 최대 질문 수\n",
    "- save_forder = 결과를 저장할 폴더 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[only_positive] \n",
      "Total Input Tokens: 994835\n",
      "Total Output Tokens: 4614\n",
      "\n",
      "[with_negative] \n",
      "Total Input Tokens: 3493302\n",
      "Total Output Tokens: 4614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create(purpose='only_positive', \n",
    "       model=\"gpt-4o-mini\", \n",
    "       user_count=1000, \n",
    "       max_question=15, \n",
    "       max_history=60, \n",
    "       save_folder=\"1~1000\",\n",
    "       start_count=0\n",
    "       )\n",
    "\n",
    "create(purpose='with_negative', \n",
    "       model=\"gpt-4o-mini\", \n",
    "       user_count=1000, \n",
    "       max_question=15, \n",
    "       max_history=60, \n",
    "       save_folder=\"1~1000\",\n",
    "       start_count=0\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coongya11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
