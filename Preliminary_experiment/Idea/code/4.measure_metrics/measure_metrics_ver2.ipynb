{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measure_metrics 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "def read_hidden_positions(file_path):\n",
    "    \"\"\"\n",
    "    question 정답 위치 정보를 읽어오는 함수\n",
    "    \"\"\"\n",
    "    hidden_positions = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = re.match(r'U(\\d+)\\s*:\\s*\\[(.*?)\\]', line.strip())\n",
    "            if match:\n",
    "                user_id = int(match.group(1))   # 사용자 ID 추출\n",
    "                positions = list(map(int, match.group(2).split(','))) # 정답 위치 리스트 추출\n",
    "                hidden_positions[user_id] = positions\n",
    "    return hidden_positions\n",
    "\n",
    "def read_predicted_rankings(file_path):\n",
    "    \"\"\"\n",
    "    예측된 순위를 읽어오는 함수\n",
    "    - 파일 내에서 [U숫자] 단위로 split 한 다음\n",
    "      각 블록 내의 매 라인에서 \"Question X: ...\" 패턴을 찾아 순위를 파싱\n",
    "    \"\"\"\n",
    "    predicted_rankings = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # [U숫자] 블록들을 추출 (user_id, user_content)\n",
    "    user_blocks = re.findall(r'\\[U(\\d+)\\](.*?)(?=\\n\\[U\\d+\\]|\\Z)', content, flags=re.DOTALL)\n",
    "\n",
    "    for user_block in user_blocks:\n",
    "        user_id = int(user_block[0])\n",
    "        user_content = user_block[1]\n",
    "\n",
    "        lines = user_content.strip().split('\\n')\n",
    "        user_questions = {}\n",
    "\n",
    "        for line in lines:\n",
    "            # \"Question 1: 19, 4, ...\" 형태의 패턴\n",
    "            match_q = re.match(r'Question\\s+(\\d+)\\s*:\\s*(.*)', line.strip())\n",
    "            if match_q:\n",
    "                q_num = int(match_q.group(1))\n",
    "                ranking_str = match_q.group(2).strip()\n",
    "                \n",
    "                # 쉼표로 split -> 정수 변환\n",
    "                # 이 때, x.strip()이 빈 문자열인 경우를 필터링해서 예외 방지\n",
    "                ranking = [\n",
    "                    int(x.strip())\n",
    "                    for x in ranking_str.split(',')\n",
    "                    if x.strip()  # 빈 문자열(\"\")은 건너뛴다\n",
    "                ]\n",
    "                \n",
    "                user_questions[q_num] = ranking\n",
    "\n",
    "        predicted_rankings[user_id] = user_questions\n",
    "\n",
    "    return predicted_rankings\n",
    "\n",
    "\n",
    "def ndcg_for_rank(rank, k):\n",
    "    \"\"\"\n",
    "    랭크(rank: 1-based)가 k 이내일 때의 nDCG 기여도를 계산하는 함수.\n",
    "    rank가 k보다 크면 0을 반환.\n",
    "    \"\"\"\n",
    "    if rank <= k:\n",
    "        # DCG 공식: 1 / log2(rank + 1)\n",
    "        return 1 / math.log2(rank + 1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def compute_metrics(rankings, hidden_positions):\n",
    "    user_ndcg = {}\n",
    "    question_ndcg_5 = []\n",
    "    question_ndcg_10 = []\n",
    "    question_auc = []\n",
    "    question_mrr = []\n",
    "\n",
    "    total_questions = 0\n",
    "    total_correct_top1 = 0\n",
    "\n",
    "    user_accuracy = {}\n",
    "    user_auc = {}\n",
    "    user_mrr = {}\n",
    "\n",
    "    user_correct_counts = {}\n",
    "    user_total_questions = {}\n",
    "\n",
    "    for user_id, user_rankings in rankings.items():\n",
    "        if user_id not in hidden_positions:\n",
    "            continue\n",
    "\n",
    "        user_positions = hidden_positions[user_id]\n",
    "        ndcg5_values = []\n",
    "        ndcg10_values = []\n",
    "        auc_values = []\n",
    "        mrr_values = []\n",
    "\n",
    "        correct_top1_count = 0\n",
    "        total_user_questions = 0\n",
    "\n",
    "        for q_num, ranking in user_rankings.items():\n",
    "            if q_num - 1 >= len(user_positions):\n",
    "                continue\n",
    "\n",
    "            correct_item = user_positions[q_num - 1]\n",
    "            total_questions += 1\n",
    "            total_user_questions += 1\n",
    "\n",
    "            if correct_item in ranking:\n",
    "                rank = ranking.index(correct_item) + 1\n",
    "                ndcg5 = ndcg_for_rank(rank, 5)\n",
    "                ndcg10 = ndcg_for_rank(rank, 10)\n",
    "                rr = 1.0 / rank\n",
    "                auc = (len(ranking) - rank) / (len(ranking) - 1) if len(ranking) > 1 else 0\n",
    "            else:\n",
    "                ndcg5 = 0\n",
    "                ndcg10 = 0\n",
    "                rr = 0\n",
    "                auc = 0\n",
    "\n",
    "            ndcg5_values.append((q_num, ndcg5))\n",
    "            ndcg10_values.append((q_num, ndcg10))\n",
    "            auc_values.append((q_num, auc))\n",
    "            mrr_values.append((q_num, rr))\n",
    "\n",
    "            question_ndcg_5.append(ndcg5)\n",
    "            question_ndcg_10.append(ndcg10)\n",
    "            question_auc.append(auc)\n",
    "            question_mrr.append(rr)\n",
    "\n",
    "            if len(ranking) > 0 and ranking[0] == correct_item:\n",
    "                correct_top1_count += 1\n",
    "                total_correct_top1 += 1\n",
    "\n",
    "        avg_ndcg5 = sum(v for _, v in ndcg5_values) / len(ndcg5_values) if ndcg5_values else 0\n",
    "        avg_ndcg10 = sum(v for _, v in ndcg10_values) / len(ndcg10_values) if ndcg10_values else 0\n",
    "        avg_auc = sum(v for _, v in auc_values) / len(auc_values) if auc_values else 0\n",
    "        avg_mrr = sum(v for _, v in mrr_values) / len(mrr_values) if mrr_values else 0\n",
    "        accuracy = correct_top1_count / total_user_questions if total_user_questions else 0\n",
    "\n",
    "        user_ndcg[user_id] = {\n",
    "            'ndcg5': ndcg5_values,\n",
    "            'ndcg10': ndcg10_values,\n",
    "            'auc': auc_values,\n",
    "            'mrr': mrr_values,\n",
    "            'avg_ndcg5': avg_ndcg5,\n",
    "            'avg_ndcg10': avg_ndcg10,\n",
    "            'avg_auc': avg_auc,\n",
    "            'avg_mrr': avg_mrr\n",
    "        }\n",
    "        user_accuracy[user_id] = accuracy\n",
    "        user_auc[user_id] = avg_auc\n",
    "        user_mrr[user_id] = avg_mrr\n",
    "        user_correct_counts[user_id] = correct_top1_count\n",
    "        user_total_questions[user_id] = total_user_questions\n",
    "\n",
    "    overall_user_ndcg5 = sum(v['avg_ndcg5'] for v in user_ndcg.values()) / len(user_ndcg) if user_ndcg else 0\n",
    "    overall_user_ndcg10 = sum(v['avg_ndcg10'] for v in user_ndcg.values()) / len(user_ndcg) if user_ndcg else 0\n",
    "    overall_question_ndcg5 = sum(question_ndcg_5) / len(question_ndcg_5) if question_ndcg_5 else 0\n",
    "    overall_question_ndcg10 = sum(question_ndcg_10) / len(question_ndcg_10) if question_ndcg_10 else 0\n",
    "\n",
    "    overall_user_auc = sum(user_auc.values()) / len(user_auc) if user_auc else 0\n",
    "    overall_question_auc = sum(question_auc) / len(question_auc) if question_auc else 0\n",
    "\n",
    "    overall_user_mrr = sum(user_mrr.values()) / len(user_mrr) if user_mrr else 0\n",
    "    overall_question_mrr = sum(question_mrr) / len(question_mrr) if question_mrr else 0\n",
    "\n",
    "    overall_user_accuracy = sum(user_accuracy.values()) / len(user_accuracy) if user_accuracy else 0\n",
    "    overall_question_accuracy = total_correct_top1 / total_questions if total_questions else 0\n",
    "\n",
    "    return (\n",
    "        user_ndcg,\n",
    "        overall_user_ndcg5,\n",
    "        overall_user_ndcg10,\n",
    "        overall_question_ndcg5,\n",
    "        overall_question_ndcg10,\n",
    "        user_accuracy,\n",
    "        overall_user_accuracy,\n",
    "        overall_question_accuracy,\n",
    "        total_correct_top1,\n",
    "        total_questions,\n",
    "        user_correct_counts,\n",
    "        user_total_questions,\n",
    "        overall_user_auc,\n",
    "        overall_question_auc,\n",
    "        overall_user_mrr,\n",
    "        overall_question_mrr\n",
    "    )\n",
    "\n",
    "def write_results(\n",
    "    file_path,\n",
    "    user_ndcg,\n",
    "    overall_user_ndcg5,\n",
    "    overall_user_ndcg10,\n",
    "    overall_question_ndcg5,\n",
    "    overall_question_ndcg10,\n",
    "    user_accuracy,\n",
    "    overall_user_accuracy,\n",
    "    overall_question_accuracy,\n",
    "    total_correct_top1,\n",
    "    total_questions,\n",
    "    user_correct_counts,\n",
    "    user_total_questions,\n",
    "    overall_user_auc,\n",
    "    overall_question_auc,\n",
    "    overall_user_mrr,\n",
    "    overall_question_mrr\n",
    "):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f'전체 USER 평균 nDCG@5  : {overall_user_ndcg5:.3f}\\n')\n",
    "        f.write(f'전체 USER 평균 nDCG@10 : {overall_user_ndcg10:.3f}\\n')\n",
    "        f.write(f'전체 USER 평균 AUC  : {overall_user_auc:.3f}\\n')\n",
    "        f.write(f'전체 USER 평균 MRR : {overall_user_mrr:.3f}\\n')\n",
    "        f.write(f'전체 USER 평균 Accuracy : {overall_user_accuracy:.3f}\\n\\n')\n",
    "        f.write(f'전체 Question nDCG@5  : {overall_question_ndcg5:.3f}\\n')\n",
    "        f.write(f'전체 Question nDCG@10 : {overall_question_ndcg10:.3f}\\n')\n",
    "        f.write(f'전체 Question AUC : {overall_question_auc:.3f}\\n')\n",
    "        f.write(f'전체 Question MRR : {overall_question_mrr:.3f}\\n\\n')\n",
    "        f.write(f'전체 Question Accuracy : {overall_question_accuracy:.3f} ({total_correct_top1} / {total_questions})\\n')\n",
    "        f.write('\\n-----------------------------------------------------------\\n\\n')\n",
    "\n",
    "        for user_id in sorted(user_ndcg.keys()):\n",
    "            ndcg_info = user_ndcg[user_id]\n",
    "            avg_ndcg5 = ndcg_info['avg_ndcg5']\n",
    "            avg_ndcg10 = ndcg_info['avg_ndcg10']\n",
    "            avg_auc = ndcg_info['avg_auc']\n",
    "            avg_mrr = ndcg_info['avg_mrr']\n",
    "            accuracy = user_accuracy.get(user_id, 0)\n",
    "            correct_top1_count = user_correct_counts.get(user_id, 0)\n",
    "            total_user_q = user_total_questions.get(user_id, 0)\n",
    "\n",
    "            f.write(\n",
    "                f'[U{user_id}] : 평균 nDCG@5 : {avg_ndcg5:.3f}  |  '\n",
    "                f'평균 nDCG@10 : {avg_ndcg10:.3f}  |  '\n",
    "                f'AUC : {avg_auc:.3f}  |  '\n",
    "                f'MRR : {avg_mrr:.3f}  |  '\n",
    "                f'Accuracy : {accuracy:.3f} ({correct_top1_count}/{total_user_q})\\n'\n",
    "            )\n",
    "\n",
    "            ndcg5_dict = dict(ndcg_info['ndcg5'])\n",
    "            ndcg10_dict = dict(ndcg_info['ndcg10'])\n",
    "\n",
    "            all_q_nums = sorted(set(ndcg5_dict.keys()) | set(ndcg10_dict.keys()))\n",
    "            for q_num in all_q_nums:\n",
    "                ndcg5_val = ndcg5_dict.get(q_num, 0.0)\n",
    "                ndcg10_val = ndcg10_dict.get(q_num, 0.0)\n",
    "                f.write(\n",
    "                    f'  - Question {q_num} : '\n",
    "                    f'nDCG@5 = {ndcg5_val:.3f} | '\n",
    "                    f'nDCG@10 = {ndcg10_val:.3f}\\n'\n",
    "                )\n",
    "            f.write('\\n')\n",
    "\n",
    "def measure_metrics(target_file, target_folder, purpose):\n",
    "    \"\"\"\n",
    "    metric 측정 main 함수\n",
    "    \"\"\"\n",
    "    target_folder = f'../../prompts/{target_folder}'\n",
    "\n",
    "    if purpose == 'new_negative':\n",
    "        hidden_file = f'{target_folder}/metadata/hidden_positions.txt'\n",
    "    else:\n",
    "        hidden_file = f'{target_folder}/{purpose}/metadata/hidden_positions.txt'\n",
    "\n",
    "    output_file = f'{target_file.replace(\".txt\", \"\")}_metrics.txt'\n",
    "\n",
    "    predicted_rankings = read_predicted_rankings(\n",
    "        os.path.join('../../results/gpt_result', target_file)\n",
    "    )\n",
    "    hidden_positions = read_hidden_positions(hidden_file)\n",
    "\n",
    "    (\n",
    "        user_ndcg,\n",
    "        overall_user_ndcg5,\n",
    "        overall_user_ndcg10,\n",
    "        overall_question_ndcg5,\n",
    "        overall_question_ndcg10,\n",
    "        user_accuracy,\n",
    "        overall_user_accuracy,\n",
    "        overall_question_accuracy,\n",
    "        total_correct_top1,\n",
    "        total_questions,\n",
    "        user_correct_counts,\n",
    "        user_total_questions,\n",
    "        overall_user_auc,\n",
    "        overall_question_auc,\n",
    "        overall_user_mrr,\n",
    "        overall_question_mrr\n",
    "    ) = compute_metrics(predicted_rankings, hidden_positions)\n",
    "\n",
    "    os.makedirs(os.path.join('../../results', 'metrics'), exist_ok=True)\n",
    "    write_results(\n",
    "        os.path.join('../../results', 'metrics', output_file),\n",
    "        user_ndcg,\n",
    "        overall_user_ndcg5,\n",
    "        overall_user_ndcg10,\n",
    "        overall_question_ndcg5,\n",
    "        overall_question_ndcg10,\n",
    "        user_accuracy,\n",
    "        overall_user_accuracy,\n",
    "        overall_question_accuracy,\n",
    "        total_correct_top1,\n",
    "        total_questions,\n",
    "        user_correct_counts,\n",
    "        user_total_questions,\n",
    "        overall_user_auc,\n",
    "        overall_question_auc,\n",
    "        overall_user_mrr,\n",
    "        overall_question_mrr\n",
    "    )\n",
    "    print(f'{output_file} 생성 완료 (대상 : {target_file})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KCC-ranking]Adressa only_title (final)_metrics.txt 생성 완료 (대상 : [KCC-ranking]Adressa only_title (final).txt)\n"
     ]
    }
   ],
   "source": [
    "measure_metrics(target_file='[KCC-ranking]Adressa only_title (final).txt', target_folder = \"Adressa/KCC/ranking/only_title\", purpose='only_positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KCC-ranking]Adressa subcate (final)_metrics.txt 생성 완료 (대상 : [KCC-ranking]Adressa subcate (final).txt)\n"
     ]
    }
   ],
   "source": [
    "measure_metrics(target_file='[KCC-ranking]Adressa subcate (final).txt', target_folder = \"Adressa/KCC/ranking/subcate\", purpose='only_positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KCC-ranking]Adressa cate (final)_metrics.txt 생성 완료 (대상 : [KCC-ranking]Adressa cate (final).txt)\n"
     ]
    }
   ],
   "source": [
    "measure_metrics(target_file='[KCC-ranking]Adressa cate (final).txt', target_folder = \"Adressa/KCC/ranking/cate\", purpose='only_positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KCC-ranking]Adressa both(subcate) (final)_metrics.txt 생성 완료 (대상 : [KCC-ranking]Adressa both(subcate) (final).txt)\n"
     ]
    }
   ],
   "source": [
    "measure_metrics(target_file='[KCC-ranking]Adressa both(subcate) (final).txt', target_folder = \"Adressa/KCC/ranking/both(subcate)\", purpose='only_positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250430]cate,title recurrent summary, negative_metrics.txt 생성 완료 (대상 : [250430]cate,title recurrent summary, negative.txt)\n"
     ]
    }
   ],
   "source": [
    "measure_metrics(target_file='[250430]cate,title recurrent summary, negative.txt', target_folder = \"MIND/summary/[250429]cate,title recurrent summary, negative\", purpose='with_negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250430]cate,title one summary, negative, summary-subcate on_metrics.txt 생성 완료 (대상 : [250430]cate,title one summary, negative, summary-subcate on.txt)\n"
     ]
    }
   ],
   "source": [
    "measure_metrics(target_file='[250430]cate,title one summary, negative, summary-subcate on.txt', target_folder = \"MIND/summary/[250429]cate,title one summary, negative, summary-subcate on\", purpose='with_negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coongya11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
