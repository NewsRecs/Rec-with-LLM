{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measure_metrics 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "def read_hidden_positions(file_path):\n",
    "    \"\"\"\n",
    "    question 정답 위치 정보를 읽어오는 함수\n",
    "    \"\"\"\n",
    "    hidden_positions = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = re.match(r'U(\\d+)\\s*:\\s*\\[(.*?)\\]', line.strip())\n",
    "            if match:\n",
    "                user_id = int(match.group(1))   # 사용자 ID 추출\n",
    "                positions = list(map(int, match.group(2).split(','))) # 정답 위치 리스트 추출\n",
    "                hidden_positions[user_id] = positions\n",
    "    return hidden_positions\n",
    "\n",
    "def read_predicted_rankings(file_path):\n",
    "    \"\"\"\n",
    "    예측된 순위를 읽어오는 함수\n",
    "    - 파일 내에서 [U숫자] 단위로 split 한 다음\n",
    "      각 블록 내의 매 라인에서 \"Question X: ...\" 패턴을 찾아 순위를 파싱\n",
    "    \"\"\"\n",
    "    predicted_rankings = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # [U숫자] 블록들을 추출 (user_id, user_content)\n",
    "    user_blocks = re.findall(r'\\[U(\\d+)\\](.*?)(?=\\n\\[U\\d+\\]|\\Z)', content, flags=re.DOTALL)\n",
    "\n",
    "    for user_block in user_blocks:\n",
    "        user_id = int(user_block[0])\n",
    "        user_content = user_block[1]\n",
    "\n",
    "        lines = user_content.strip().split('\\n')\n",
    "        user_questions = {}\n",
    "\n",
    "        for line in lines:\n",
    "            # \"Question 1: 19, 4, ...\" 형태의 패턴\n",
    "            match_q = re.match(r'Question\\s+(\\d+)\\s*:\\s*(.*)', line.strip())\n",
    "            if match_q:\n",
    "                q_num = int(match_q.group(1))\n",
    "                ranking_str = match_q.group(2).strip()\n",
    "                \n",
    "                # 쉼표로 split -> 정수 변환\n",
    "                # 이 때, x.strip()이 빈 문자열인 경우를 필터링해서 예외 방지\n",
    "                ranking = [\n",
    "                    int(x.strip())\n",
    "                    for x in ranking_str.split(',')\n",
    "                    if x.strip()  # 빈 문자열(\"\")은 건너뛴다\n",
    "                ]\n",
    "                \n",
    "                user_questions[q_num] = ranking\n",
    "\n",
    "        predicted_rankings[user_id] = user_questions\n",
    "\n",
    "    return predicted_rankings\n",
    "\n",
    "\n",
    "def ndcg_for_rank(rank, k):\n",
    "    \"\"\"\n",
    "    랭크(rank: 1-based)가 k 이내일 때의 nDCG 기여도를 계산하는 함수.\n",
    "    rank가 k보다 크면 0을 반환.\n",
    "    \"\"\"\n",
    "    if rank <= k:\n",
    "        # DCG 공식: 1 / log2(rank + 1)\n",
    "        return 1 / math.log2(rank + 1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def compute_ndcg_at_5_and_10(rankings, hidden_positions):\n",
    "    \"\"\"\n",
    "    nDCG@5, nDCG@10 및 정확도(Top1 Accuracy)를 계산하는 함수\n",
    "    \"\"\"\n",
    "    # 사용자별 nDCG 결과 저장용\n",
    "    user_ndcg = {}\n",
    "    \n",
    "    # 전체 질문 단위로 모은 nDCG (5, 10)\n",
    "    question_ndcg_5 = []\n",
    "    question_ndcg_10 = []\n",
    "\n",
    "    # 정확도 계산을 위한 변수\n",
    "    total_questions = 0\n",
    "    total_correct_top1 = 0\n",
    "\n",
    "    # 사용자별 Accuracy 계산을 위한 변수\n",
    "    user_accuracy = {}\n",
    "    user_correct_counts = {}\n",
    "    user_total_questions = {}\n",
    "\n",
    "    for user_id, user_rankings in rankings.items():\n",
    "        if user_id not in hidden_positions:   # user id가 없으면 무시\n",
    "            continue\n",
    "        \n",
    "        user_positions = hidden_positions[user_id]\n",
    "        ndcg5_values = []\n",
    "        ndcg10_values = []\n",
    "        \n",
    "        correct_top1_count = 0\n",
    "        total_user_questions = 0\n",
    "\n",
    "        for q_num, ranking in user_rankings.items():\n",
    "            # q_num-1 이 hidden_positions 내에서 유효 인덱스인지 확인\n",
    "            if q_num - 1 >= len(user_positions):\n",
    "                continue\n",
    "            \n",
    "            correct_item = user_positions[q_num - 1]  # question 정답 (1~N번 뉴스 중 하나)\n",
    "            total_questions += 1\n",
    "            total_user_questions += 1\n",
    "\n",
    "            # 정답 뉴스가 ranking 안에 있는지 확인\n",
    "            if correct_item in ranking:\n",
    "                rank = ranking.index(correct_item) + 1  # 실제 순위 (1-based)\n",
    "                ndcg5 = ndcg_for_rank(rank, 5)\n",
    "                ndcg10 = ndcg_for_rank(rank, 10)\n",
    "            else:\n",
    "                ndcg5 = 0\n",
    "                ndcg10 = 0\n",
    "\n",
    "            ndcg5_values.append((q_num, ndcg5))\n",
    "            ndcg10_values.append((q_num, ndcg10))\n",
    "\n",
    "            question_ndcg_5.append(ndcg5)\n",
    "            question_ndcg_10.append(ndcg10)\n",
    "\n",
    "            # 정확도 (Top1) 체크\n",
    "            if len(ranking) > 0 and ranking[0] == correct_item:\n",
    "                correct_top1_count += 1\n",
    "                total_correct_top1 += 1\n",
    "\n",
    "        # 사용자별 평균 nDCG@5, nDCG@10\n",
    "        avg_ndcg5 = sum(ndcg for _, ndcg in ndcg5_values) / len(ndcg5_values) if ndcg5_values else 0\n",
    "        avg_ndcg10 = sum(ndcg for _, ndcg in ndcg10_values) / len(ndcg10_values) if ndcg10_values else 0\n",
    "\n",
    "        # 사용자별 Accuracy\n",
    "        accuracy = correct_top1_count / total_user_questions if total_user_questions else 0\n",
    "\n",
    "        user_ndcg[user_id] = {\n",
    "            'ndcg5': ndcg5_values,\n",
    "            'ndcg10': ndcg10_values,\n",
    "            'avg_ndcg5': avg_ndcg5,\n",
    "            'avg_ndcg10': avg_ndcg10\n",
    "        }\n",
    "        user_accuracy[user_id] = accuracy\n",
    "        user_correct_counts[user_id] = correct_top1_count\n",
    "        user_total_questions[user_id] = total_user_questions\n",
    "\n",
    "    # 전체 USER 평균 nDCG@5, nDCG@10\n",
    "    if user_ndcg:\n",
    "        overall_user_ndcg5 = sum(v['avg_ndcg5'] for v in user_ndcg.values()) / len(user_ndcg)\n",
    "        overall_user_ndcg10 = sum(v['avg_ndcg10'] for v in user_ndcg.values()) / len(user_ndcg)\n",
    "    else:\n",
    "        overall_user_ndcg5 = 0\n",
    "        overall_user_ndcg10 = 0\n",
    "\n",
    "    # 전체 Question nDCG@5, nDCG@10\n",
    "    if question_ndcg_5:\n",
    "        overall_question_ndcg5 = sum(question_ndcg_5) / len(question_ndcg_5)\n",
    "    else:\n",
    "        overall_question_ndcg5 = 0\n",
    "    if question_ndcg_10:\n",
    "        overall_question_ndcg10 = sum(question_ndcg_10) / len(question_ndcg_10)\n",
    "    else:\n",
    "        overall_question_ndcg10 = 0\n",
    "\n",
    "    # 전체 USER 평균 Accuracy (사용자별 Accuracy 평균)\n",
    "    overall_user_accuracy = sum(user_accuracy.values()) / len(user_accuracy) if user_accuracy else 0\n",
    "    # 전체 Question Accuracy (모든 Question 중 Top1 정답 비율)\n",
    "    overall_question_accuracy = total_correct_top1 / total_questions if total_questions else 0\n",
    "\n",
    "    return (\n",
    "        user_ndcg,\n",
    "        overall_user_ndcg5,\n",
    "        overall_user_ndcg10,\n",
    "        overall_question_ndcg5,\n",
    "        overall_question_ndcg10,\n",
    "        user_accuracy,\n",
    "        overall_user_accuracy,\n",
    "        overall_question_accuracy,\n",
    "        total_correct_top1,\n",
    "        total_questions,\n",
    "        user_correct_counts,\n",
    "        user_total_questions\n",
    "    )\n",
    "\n",
    "\n",
    "def write_results(\n",
    "    file_path,\n",
    "    user_ndcg,\n",
    "    overall_user_ndcg5,\n",
    "    overall_user_ndcg10,\n",
    "    overall_question_ndcg5,\n",
    "    overall_question_ndcg10,\n",
    "    user_accuracy,\n",
    "    overall_user_accuracy,\n",
    "    overall_question_accuracy,\n",
    "    total_correct_top1,\n",
    "    total_questions,\n",
    "    user_correct_counts,\n",
    "    user_total_questions\n",
    "):\n",
    "    \"\"\"\n",
    "    metrics 결과 작성 함수\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # 전체 요약\n",
    "        f.write(f'전체 USER 평균 nDCG@5  : {overall_user_ndcg5:.3f}\\n')\n",
    "        f.write(f'전체 USER 평균 nDCG@10 : {overall_user_ndcg10:.3f}\\n')\n",
    "        f.write(f'전체 Question nDCG@5  : {overall_question_ndcg5:.3f}\\n')\n",
    "        f.write(f'전체 Question nDCG@10 : {overall_question_ndcg10:.3f}\\n\\n')\n",
    "\n",
    "        f.write(f'전체 USER 평균 Accuracy : {overall_user_accuracy:.3f}\\n')\n",
    "        f.write(f'전체 Question Accuracy : {overall_question_accuracy:.3f} ({total_correct_top1} / {total_questions})\\n')\n",
    "        f.write('\\n-----------------------------------------------------------\\n\\n')\n",
    "\n",
    "        # 사용자별 상세\n",
    "        for user_id in sorted(user_ndcg.keys()):\n",
    "            ndcg_info = user_ndcg[user_id]\n",
    "            avg_ndcg5 = ndcg_info['avg_ndcg5']\n",
    "            avg_ndcg10 = ndcg_info['avg_ndcg10']\n",
    "            \n",
    "            accuracy = user_accuracy.get(user_id, 0)\n",
    "            correct_top1_count = user_correct_counts.get(user_id, 0)\n",
    "            total_user_q = user_total_questions.get(user_id, 0)\n",
    "\n",
    "            f.write(\n",
    "                f'[U{user_id}] : 평균 nDCG@5 : {avg_ndcg5:.3f}  |  '\n",
    "                f'평균 nDCG@10 : {avg_ndcg10:.3f}  |  '\n",
    "                f'Accuracy : {accuracy:.3f} ({correct_top1_count}/{total_user_q})\\n'\n",
    "            )\n",
    "\n",
    "            # (q_num, ndcg_val) 리스트를 딕셔너리로 변환\n",
    "            ndcg5_dict = dict(ndcg_info['ndcg5'])   # { q_num: nDCG@5, ... }\n",
    "            ndcg10_dict = dict(ndcg_info['ndcg10']) # { q_num: nDCG@10, ... }\n",
    "\n",
    "            # 두 딕셔너리에 있는 모든 question 번호를 합쳐서 정렬\n",
    "            all_q_nums = sorted(set(ndcg5_dict.keys()) | set(ndcg10_dict.keys()))\n",
    "            \n",
    "            for q_num in all_q_nums:\n",
    "                ndcg5_val = ndcg5_dict.get(q_num, 0.0)\n",
    "                ndcg10_val = ndcg10_dict.get(q_num, 0.0)\n",
    "                # 원하는 출력 형식 예: \"  - Question 1 : nDCG@5 = 0.000 | nDCG@10 = 0.000\"\n",
    "                f.write(\n",
    "                    f'  - Question {q_num} : '\n",
    "                    f'nDCG@5 = {ndcg5_val:.3f} | '\n",
    "                    f'nDCG@10 = {ndcg10_val:.3f}\\n'\n",
    "                )\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def measure_metrics(target_file, target_folder, purpose):\n",
    "    \"\"\"\n",
    "    metric 측정 main 함수\n",
    "    \"\"\"\n",
    "    target_folder = f'../../prompts/{target_folder}'\n",
    "\n",
    "    # hidden_positions.txt 경로 설정\n",
    "    if purpose == 'new_negative':\n",
    "        hidden_file = f'{target_folder}/metadata/hidden_positions.txt'\n",
    "    else:\n",
    "        hidden_file = f'{target_folder}/{purpose}/metadata/hidden_positions.txt'\n",
    "    \n",
    "    output_file = f'{target_file.replace(\".txt\",\"\")}_metrics.txt'\n",
    "    \n",
    "    # 1) 예측된 순위 읽기\n",
    "    predicted_rankings = read_predicted_rankings(\n",
    "        os.path.join('../../results/gpt_result', target_file)\n",
    "    )\n",
    "    # 2) 정답 위치 읽기\n",
    "    hidden_positions = read_hidden_positions(hidden_file)\n",
    "    # 3) nDCG@5, nDCG@10 및 Accuracy 계산\n",
    "    (\n",
    "        user_ndcg,\n",
    "        overall_user_ndcg5,\n",
    "        overall_user_ndcg10,\n",
    "        overall_question_ndcg5,\n",
    "        overall_question_ndcg10,\n",
    "        user_accuracy,\n",
    "        overall_user_accuracy,\n",
    "        overall_question_accuracy,\n",
    "        total_correct_top1,\n",
    "        total_questions,\n",
    "        user_correct_counts,\n",
    "        user_total_questions\n",
    "    ) = compute_ndcg_at_5_and_10(predicted_rankings, hidden_positions)\n",
    "\n",
    "    # 4) 결과를 파일로 출력\n",
    "    os.makedirs(os.path.join('../../results', 'metrics'), exist_ok=True)\n",
    "    write_results(\n",
    "        os.path.join('../../results', 'metrics', output_file),\n",
    "        user_ndcg,\n",
    "        overall_user_ndcg5,\n",
    "        overall_user_ndcg10,\n",
    "        overall_question_ndcg5,\n",
    "        overall_question_ndcg10,\n",
    "        user_accuracy,\n",
    "        overall_user_accuracy,\n",
    "        overall_question_accuracy,\n",
    "        total_correct_top1,\n",
    "        total_questions,\n",
    "        user_correct_counts,\n",
    "        user_total_questions\n",
    "    )\n",
    "    print(f'{output_file} 생성 완료 (대상 : {target_file})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250325] negative_ratio_80_metrics.txt 생성 완료 (대상 : [250325] negative_ratio_80.txt)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "measure_metrics(target_file='[250325] negative_ratio_80.txt', target_folder = \"[top1] history_ratio 0.8\", purpose='with_negative')\n",
    "# measure_metrics(target_file='[250318] norway_positive2.txt', target_folder = \"[top1] norway_ns4\", purpose='only_positive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250310] positive_ns4_fine(40,15)_across(negative_model)_metrics.txt 생성 완료 (대상 : [250310] positive_ns4_fine(40,15)_across(negative_model).txt)\n",
      "[250310] negative_ns4_fine(40,15)_across(positive_model)_metrics.txt 생성 완료 (대상 : [250310] negative_ns4_fine(40,15)_across(positive_model).txt)\n"
     ]
    }
   ],
   "source": [
    "measure_metrics(target_file='[250310] positive_ns4_fine(40,15)_across(negative_model).txt', target_folder = \"[top1] test_ns4\", purpose='only_positive')\n",
    "measure_metrics(target_file='[250310] negative_ns4_fine(40,15)_across(positive_model).txt', target_folder = \"[top1] test_ns4\", purpose='with_negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coongya11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
