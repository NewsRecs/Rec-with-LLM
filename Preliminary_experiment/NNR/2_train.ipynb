{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :  <class 'model.NAML.NAML'>\n",
      "config :  <class 'config.NAMLConfig'>\n",
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from dataset import BaseDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from config import model_name \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from evaluate import evaluate\n",
    "import importlib\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "try:\n",
    "    # getattr 함수로 model.model_name 모듈 안에서 이름이 model_nam\n",
    "    # e인 클래스(__init__.py에서서)를 가져옴\n",
    "    Model = getattr(importlib.import_module(f\"model.{model_name}\"), model_name)\n",
    "    config = getattr(importlib.import_module('config'), f\"{model_name}Config\")\n",
    "except AttributeError:\n",
    "    print(f\"{model_name} not included!\")\n",
    "    exit()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Model : ', Model)\n",
    "print('config : ', config)\n",
    "print('device : ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=config.early_stop_patience):\n",
    "        # 조기 종료 기준을 위한 patience 설정\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        val_loss가 감소했는지(=성능이 좋아졌는지) 확인\n",
    "        => 좋아졌다면 self.best_loss 갱신, counter=0\n",
    "        => 아니라면 counter += 1\n",
    "        => counter >= patience면 early_stop\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss:\n",
    "            early_stop = False\n",
    "            get_better = True\n",
    "            self.counter = 0\n",
    "            self.best_loss = val_loss\n",
    "        else:\n",
    "            get_better = False\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                early_stop = True\n",
    "            else:\n",
    "                early_stop = False\n",
    "        if np.isnan(val_loss):\n",
    "            early_stop = True\n",
    "        return early_stop, get_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def latest_checkpoint(directory):\n",
    "#     # 가장 최신 체크포인트 찾기 (특정 형식에 맞는 파일 찾기)\n",
    "#     if not os.path.exists(directory):\n",
    "#         return None\n",
    "#     all_checkpoints = {\n",
    "#         int(x.split('.')[0].split('-')[1]): x\n",
    "#         for x in os.listdir(directory)\n",
    "#         if (x.split('.')[0].split('-')[2] == config.candidate_type)\n",
    "#         if (x.split('.')[0].split('-')[3] == config.our_type)\n",
    "#         if (x.split('.')[0].split('-')[4] == config.loss_function)\n",
    "#     }\n",
    "#     if not all_checkpoints:\n",
    "#         return None\n",
    "#     return os.path.join(directory,\n",
    "#                         all_checkpoints[max(all_checkpoints.keys())])\n",
    "\n",
    "\n",
    "# def latest_checkpoint(directory):\n",
    "#     \"\"\"\n",
    "#     디렉토리 내에서\n",
    "#     '{experiment_data}_ep{epoch}.ckpt' 형태의\n",
    "#     가장 큰 epoch 번호를 가진 체크포인트 파일을 찾아 반환.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(directory):\n",
    "#         return None\n",
    "\n",
    "#     all_checkpoints = {}\n",
    "#     for x in os.listdir(directory):\n",
    "#         # 파일명 예: MyExp_ep3.ckpt\n",
    "#         if x.endswith('.ckpt'):\n",
    "#             parts = x.split('_ep')\n",
    "#             if len(parts) != 2:\n",
    "#                 continue\n",
    "#             exp_data_part = parts[0]  # 예: \"MyExp\"\n",
    "#             ep_part = parts[1].split('.')[0]  # \"3\"\n",
    "#             if exp_data_part == config.experiment_data:\n",
    "#                 try:\n",
    "#                     epoch_num = int(ep_part)\n",
    "#                     all_checkpoints[epoch_num] = x\n",
    "#                 except ValueError:\n",
    "#                     continue\n",
    "\n",
    "#     if not all_checkpoints:\n",
    "#         return None\n",
    "\n",
    "#     # 가장 큰 epoch 번호\n",
    "#     latest_epoch = max(all_checkpoints.keys())\n",
    "#     return os.path.join(directory, all_checkpoints[latest_epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    \"\"\"\n",
    "    Format elapsed time string.\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    elapsed_time = now - since\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    모델 학습을 위한 train 함수 정의\n",
    "    \"\"\"\n",
    "\n",
    "    # 결과 파일 이름 설정\n",
    "    result_file = f\"./results/{model_name}/{config.experiment_data}.txt\"\n",
    "    if not os.path.exists('checkpoint'):\n",
    "        os.makedirs('checkpoint')\n",
    "\n",
    "\n",
    "    # 사전 학습된 임베딩 불러오기\n",
    "    try:\n",
    "        pretrained_word_embedding = torch.from_numpy(\n",
    "             np.load(f'{config.data_folder}/pretrained_word_embedding.npy')\n",
    "             ).float()\n",
    "    except FileNotFoundError:\n",
    "        pretrained_word_embedding = None\n",
    "\n",
    "\n",
    "    if model_name == 'DKN':\n",
    "        # DKN 모델의 경우 추가 임베딩 불러오기 (entity 및 context 임베딩)\n",
    "        try:\n",
    "            pretrained_entity_embedding = torch.from_numpy(\n",
    "                np.load(\n",
    "                    f'{config.data_folder}/pretrained_entity_embedding.npy')).float()\n",
    "        except FileNotFoundError:\n",
    "            pretrained_entity_embedding = None\n",
    "\n",
    "        try:\n",
    "            pretrained_context_embedding = torch.from_numpy(\n",
    "                np.load(\n",
    "                    f'{config.data_folder}/pretrained_context_embedding.npy')).float()\n",
    "        except FileNotFoundError:\n",
    "            pretrained_context_embedding = None\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = Model(config, pretrained_word_embedding,\n",
    "                      pretrained_entity_embedding,\n",
    "                      pretrained_context_embedding).to(device)\n",
    "    elif model_name == 'Exp1':\n",
    "        # Exp1 모델의 경우 앙상블 사용\n",
    "        models = nn.ModuleList([\n",
    "            Model(config, pretrained_word_embedding).to(device)\n",
    "            for _ in range(config.ensemble_factor)\n",
    "        ])\n",
    "    elif model_name == 'Exp2':\n",
    "        model = Model(config).to(device)\n",
    "    else:\n",
    "        model = Model(config, pretrained_word_embedding).to(device)\n",
    "\n",
    "\n",
    "    if model_name != 'Exp1':\n",
    "        print(model)\n",
    "    else:\n",
    "        print(models[0])\n",
    "\n",
    " \n",
    "    \n",
    "    dataset = BaseDataset(f'{config.data_folder}/train/{config.experiment_data}.tsv',   # behaviors_path\n",
    "                          f'{config.data_folder}/news_parsed.tsv',                      # news_path\n",
    "                          f'{config.data_folder}/roberta')                              # roberta_embedding_dir\n",
    "\n",
    "    print(f\"Load training dataset with size {len(dataset)}.\")\n",
    "\n",
    "    dataloader = iter(\n",
    "        DataLoader(dataset,\n",
    "                   batch_size=config.batch_size,\n",
    "                   shuffle=True,\n",
    "                   num_workers=config.num_workers,\n",
    "                   drop_last=True,\n",
    "                   pin_memory=True))\n",
    "    \n",
    "\n",
    "\n",
    "    # 손실 함수 및 옵티마이저 설정\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                    lr=config.learning_rate)\n",
    "    print(f\"Loss function:{config.loss_function}, NS Type: {config.candidate_type}_{config.our_type}\")\n",
    "\n",
    "    # 조기 종료 & “최고 성능” 추적\n",
    "    early_stopping = EarlyStopping()\n",
    "    best_ndcg5 = -1.0  # nDCG@5는 0~1 범위가 일반적이므로 -1로 초기화\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss_full = []\n",
    "    exhaustion_count = 0\n",
    "    step = 0\n",
    "\n",
    "    # 체크포인트 및 결과 디렉토리 생성\n",
    "    checkpoint_dir = os.path.join('./checkpoint', model_name)\n",
    "    result_dir = os.path.join('./results', model_name)\n",
    "    Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 가장 최근 체크포인트 불러오기 (없으면 None)\n",
    "    # checkpoint_path = latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    epoch_result = []\n",
    "    # if checkpoint_path is not None:\n",
    "    #     # 체크포인트에서 파라미터 불러오기\n",
    "    #     print(f\"Load saved parameters in {checkpoint_path}\")\n",
    "    #     checkpoint = torch.load(checkpoint_path)\n",
    "    #     early_stopping(checkpoint['early_stop_value'])\n",
    "    #     step = checkpoint['step']\n",
    "    #     exhaustion_count = checkpoint['exhaustion_count']\n",
    "    #     epoch_result = [x.split(' ') for x in checkpoint['epoch_result'].split('\\n')]\n",
    "    #     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    #     model.train()\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    #     메인 학습 루프\n",
    "    # -----------------------\n",
    "    total_iters = config.num_epochs * len(dataset) // config.batch_size\n",
    "\n",
    "    for i in tqdm(range(1, total_iters + 1), desc=\"Training\"):\n",
    "        try:\n",
    "            # dataloader가 반환할 수 있는 데이터(미니배치)가 남아 있는 경우\n",
    "            minibatch = next(dataloader)\n",
    "        except StopIteration:\n",
    "            # dataloader가 데이터셋의 끝에 도달하여 반환할 데이터가 없는 경우 (한 epoch 종료)\n",
    "            exhaustion_count += 1\n",
    "\n",
    "            # 검증 데이터 평가 및 결과 저장\n",
    "            model.eval()\n",
    "            val_auc, val_mrr, val_ndcg5, val_ndcg10, val_acc = evaluate(\n",
    "                model, f'{config.data_folder}')\n",
    "            model.train()\n",
    "\n",
    "            tqdm.write(\n",
    "                f\"Time {time_since(start_time)}, \"\n",
    "                f\"epoch {exhaustion_count}, batch {i}\\n\"\n",
    "                f\"validation AUC: {val_auc:.4f}, MRR: {val_mrr:.4f}, \"\n",
    "                f\"nDCG@5: {val_ndcg5:.4f}, nDCG@10: {val_ndcg10:.4f}, \"\n",
    "                f\"ACC: {val_acc:.4f}\"\n",
    "            )\n",
    "            \n",
    "            print()\n",
    "            print('┌─────────────┐')\n",
    "            print(f'│{exhaustion_count} Epoch Done!│')\n",
    "            print('└─────────────┘')\n",
    "            print()\n",
    "\n",
    "            # 로그 기록\n",
    "            epoch_result.append([\n",
    "                exhaustion_count,\n",
    "                round(val_auc, 4),\n",
    "                round(val_mrr, 4),\n",
    "                round(val_ndcg5, 4),\n",
    "                round(val_ndcg10, 4),\n",
    "                round(val_acc, 4)\n",
    "            ])\n",
    "            with open(result_file, 'w') as wf:\n",
    "                wf.write(\"Epoch\\tValidation AUC\\tValidation MRR\\tValidation nDCG@5\\tValidation nDCG@10\\tValidation ACC\\n\")\n",
    "                for row in epoch_result:\n",
    "                    wf.write(f\"{row[0]}\\t{row[1]:.4f}\\t{row[2]:.4f}\\t{row[3]:.4f}\\t{row[4]:.4f}\\t{row[5]:.4f}\\n\")\n",
    "\n",
    "            # epoch_result.append([str(val_auc),str(val_mrr),str(val_ndcg5),str(val_ndcg10)])\n",
    "            # with open(result_file,'w') as wf:\n",
    "            #     line = '\\n'.join([ ' '.join(x) for x in epoch_result])\n",
    "            #     wf.write(line)\n",
    "\n",
    "            val_loss = -sum([val_auc, val_mrr, val_ndcg5, val_ndcg10])  # 지표 합이 커질수록 좋음 → 음수화\n",
    "            early_stop, get_better = early_stopping(val_loss)\n",
    "\n",
    "            # # === “가장 성능이 좋았을 때만” 저장 ===\n",
    "            # if get_better:\n",
    "            #     checkpoint_name = f\"{config.experiment_data}_ep{exhaustion_count}.ckpt\"\n",
    "            #     save_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "            #     torch.save({\n",
    "            #         'step': step,\n",
    "            #         'exhaustion_count': exhaustion_count,\n",
    "            #         'model_state_dict': model.state_dict(),\n",
    "            #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #         'early_stop_value': val_loss,\n",
    "            #         'epoch_result': '\\n'.join(\n",
    "            #             [' '.join(map(str, x)) for x in epoch_result]\n",
    "            #         )\n",
    "            #     }, save_path)\n",
    "            #     print(f\"Model improved; saved at {save_path}\")\n",
    "\n",
    "\n",
    "            # === nDCG@5가 더 좋아졌을 때만 모델 저장 ===\n",
    "            if val_ndcg5 > best_ndcg5:\n",
    "                best_ndcg5 = val_ndcg5\n",
    "                # EarlyStopping에 사용할 loss = -nDCG@5 (클수록 좋은 지표 → 음수화해서 'loss'로 사용)\n",
    "                val_loss = -val_ndcg5\n",
    "\n",
    "                # 모델 저장\n",
    "                checkpoint_name = f\"{config.experiment_data}_ep{exhaustion_count}.ckpt\"\n",
    "                save_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "                torch.save({\n",
    "                    'epoch': exhaustion_count,\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_ndcg5': best_ndcg5\n",
    "                }, save_path)\n",
    "                tqdm.write(f\"  >>> Model improved (nDCG@5={val_ndcg5:.4f}); saved at {save_path}\")\n",
    "\n",
    "                # EarlyStopping 업데이트\n",
    "                early_stop, get_better = early_stopping(val_loss)\n",
    "            else:\n",
    "                # nDCG@5가 좋아지지 않았다면 EarlyStopping만 확인\n",
    "                val_loss = -val_ndcg5\n",
    "                early_stop, get_better = early_stopping(val_loss)\n",
    "\n",
    "            # Early stop\n",
    "            if early_stop:\n",
    "                tqdm.write(f'Epoch {exhaustion_count} Done! Early stop triggered.')\n",
    "                break\n",
    "\n",
    "            # 만약 지정된 총 epoch 수에 도달했으면 중단\n",
    "            if exhaustion_count == config.num_epochs:\n",
    "                break\n",
    "\n",
    "            # 다음 epoch 준비\n",
    "            dataloader = iter(\n",
    "                DataLoader(dataset,\n",
    "                           batch_size=config.batch_size,\n",
    "                           shuffle=True,\n",
    "                           num_workers=config.num_workers,\n",
    "                           drop_last=True,\n",
    "                           pin_memory=True))\n",
    "            try:\n",
    "                minibatch = next(dataloader)\n",
    "            except StopIteration:\n",
    "                # 혹시 데이터가 아주 작아서 바로 끝날 수도 있음\n",
    "                break\n",
    "\n",
    "        # -----------------------\n",
    "        #  배치 학습 (forward/backward)\n",
    "        # -----------------------\n",
    "        step += 1\n",
    "\n",
    "        # 모델 예측 및 손실 계산 (forward() 메서드 실행)\n",
    "        if model_name == 'LSTUR':\n",
    "            y_pred = model(\n",
    "                minibatch[\"user\"], \n",
    "                minibatch[\"clicked_news_length\"],\n",
    "                minibatch[\"candidate_news\"],\n",
    "                minibatch[\"clicked_news\"]\n",
    "                )\n",
    "        elif model_name == 'HiFiArk':\n",
    "            y_pred, regularizer_loss = model(\n",
    "                minibatch[\"candidate_news\"],\n",
    "                minibatch[\"clicked_news\"]\n",
    "                )\n",
    "        elif model_name == 'TANR':\n",
    "            y_pred, topic_classification_loss = model(\n",
    "                minibatch[\"candidate_news\"], \n",
    "                minibatch[\"clicked_news\"]\n",
    "                )\n",
    "        else:\n",
    "            y_pred = model(\n",
    "                minibatch[\"candidate_news\"], \n",
    "                minibatch[\"clicked_news\"]\n",
    "                )\n",
    "\n",
    "        y_true = torch.zeros(len(y_pred)).long().to(device)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss_full.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 값을 일정 주기로 출력\n",
    "        if i % config.num_batches_show_loss == 0:\n",
    "            tqdm.write(\n",
    "                f\"Time {time_since(start_time)}, batches {i}, \"\n",
    "                f\"current loss {loss.item():.4f}, \"\n",
    "                f\"average loss: {np.mean(loss_full):.4f}, \"\n",
    "                f\"latest average loss: {np.mean(loss_full[-256:]):.4f}\"\n",
    "            )\n",
    "            if np.isnan(loss.item()):\n",
    "                break\n",
    "\n",
    " \n",
    "\n",
    "    # -----------------------\n",
    "    #  전체 학습 끝난 후 최종 평가 (선택)\n",
    "    # -----------------------\n",
    "    # print(\"\\n=== Training Finished! Evaluating final model... ===\")\n",
    "    # model.eval()\n",
    "    # val_auc, val_mrr, val_ndcg5, val_ndcg10 = evaluate(model, f'{config.data_folder}')\n",
    "    # print(f\"Final val AUC: {val_auc:.4f}, MRR: {val_mrr:.4f}, \"\n",
    "    #       f\"nDCG@5: {val_ndcg5:.4f}, nDCG@10: {val_ndcg10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Training model NAML\n",
      "NAML(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (text_encoders): ModuleDict(\n",
      "      (title): TextEncoder(\n",
      "        (word_embedding): Embedding(330900, 100, padding_idx=0)\n",
      "        (CNN): Conv2d(1, 300, kernel_size=(3, 100), stride=(1, 1), padding=(1, 0))\n",
      "        (additive_attention): AdditiveAttention(\n",
      "          (linear): Linear(in_features=300, out_features=200, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (abstract): TextEncoder(\n",
      "        (word_embedding): Embedding(330900, 100, padding_idx=0)\n",
      "        (CNN): Conv2d(1, 300, kernel_size=(3, 100), stride=(1, 1), padding=(1, 0))\n",
      "        (additive_attention): AdditiveAttention(\n",
      "          (linear): Linear(in_features=300, out_features=200, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (element_encoders): ModuleDict(\n",
      "      (category): ElementEncoder(\n",
      "        (embedding): Embedding(128, 100, padding_idx=0)\n",
      "        (linear): Linear(in_features=100, out_features=300, bias=True)\n",
      "      )\n",
      "      (subcategory): ElementEncoder(\n",
      "        (embedding): Embedding(128, 100, padding_idx=0)\n",
      "        (linear): Linear(in_features=100, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (final_attention): AdditiveAttention(\n",
      "      (linear): Linear(in_features=300, out_features=200, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (additive_attention): AdditiveAttention(\n",
      "      (linear): Linear(in_features=300, out_features=200, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (click_predictor): DotProductClickPredictor()\n",
      ")\n",
      "Load training dataset with size 12988.\n",
      "Loss function:CEL, NS Type: random_onetype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 100/1298 [00:43<08:44,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:00:42, batches 100, current loss 1.3193, average loss: 1.3298, latest average loss: 1.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 200/1298 [01:24<07:58,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:01:24, batches 200, current loss 1.1507, average loss: 1.2492, latest average loss: 1.2492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating vectors for news: 100%|██████████| 24060/24060 [00:40<00:00, 587.84it/s]\n",
      "Calculating vectors for users: 100%|██████████| 1000/1000 [00:02<00:00, 334.45it/s]\n",
      "Calculating probabilities: 100%|██████████| 10989/10989 [00:13<00:00, 796.46it/s]\n",
      "Training:  20%|█▉        | 259/1298 [02:58<07:13,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:02:58, epoch 1, batch 260\n",
      "validation AUC: 0.6673, MRR: 0.2540, nDCG@5: 0.2435, nDCG@10: 0.3514, ACC: 0.0919\n",
      "\n",
      "┌─────────────┐\n",
      "│1 Epoch Done!│\n",
      "└─────────────┘\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 259/1298 [03:01<07:13,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >>> Model improved (nDCG@5=0.2435); saved at ./checkpoint\\NAML\\behaviors_user1000_ns4_cdNone_ep1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 300/1298 [03:18<07:19,  2.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:03:18, batches 300, current loss 0.8647, average loss: 1.1605, latest average loss: 1.1167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 400/1298 [04:00<06:36,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:04:00, batches 400, current loss 0.7779, average loss: 1.0846, latest average loss: 0.9647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 500/1298 [04:42<05:51,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:04:42, batches 500, current loss 0.7003, average loss: 1.0267, latest average loss: 0.8502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating vectors for news: 100%|██████████| 24060/24060 [00:48<00:00, 495.22it/s]\n",
      "Calculating vectors for users: 100%|██████████| 1000/1000 [00:03<00:00, 282.60it/s]\n",
      "Calculating probabilities: 100%|██████████| 10989/10989 [00:15<00:00, 702.93it/s]\n",
      "Training:  40%|███▉      | 518/1298 [06:03<05:27,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:06:03, epoch 2, batch 519\n",
      "validation AUC: 0.6768, MRR: 0.2648, nDCG@5: 0.2680, nDCG@10: 0.3605, ACC: 0.0868\n",
      "\n",
      "┌─────────────┐\n",
      "│2 Epoch Done!│\n",
      "└─────────────┘\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 518/1298 [06:05<05:27,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >>> Model improved (nDCG@5=0.2680); saved at ./checkpoint\\NAML\\behaviors_user1000_ns4_cdNone_ep2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 600/1298 [06:40<05:09,  2.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:06:40, batches 600, current loss 0.6956, average loss: 0.9813, latest average loss: 0.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 700/1298 [07:24<04:32,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:07:24, batches 700, current loss 0.4921, average loss: 0.9436, latest average loss: 0.7484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating vectors for news: 100%|██████████| 24060/24060 [00:45<00:00, 531.25it/s]\n",
      "Calculating vectors for users: 100%|██████████| 1000/1000 [00:03<00:00, 322.72it/s]\n",
      "Calculating probabilities: 100%|██████████| 10989/10989 [00:15<00:00, 724.19it/s]\n",
      "Training:  60%|█████▉    | 777/1298 [09:06<03:41,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:09:06, epoch 3, batch 778\n",
      "validation AUC: 0.6689, MRR: 0.2564, nDCG@5: 0.2572, nDCG@10: 0.3489, ACC: 0.0826\n",
      "\n",
      "┌─────────────┐\n",
      "│3 Epoch Done!│\n",
      "└─────────────┘\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 800/1298 [09:16<03:49,  2.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:09:16, batches 800, current loss 0.8553, average loss: 0.9136, latest average loss: 0.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 900/1298 [09:59<02:57,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:09:59, batches 900, current loss 0.5693, average loss: 0.8897, latest average loss: 0.7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 1000/1298 [10:42<02:13,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:10:42, batches 1000, current loss 0.7339, average loss: 0.8680, latest average loss: 0.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating vectors for news: 100%|██████████| 24060/24060 [00:42<00:00, 560.35it/s]\n",
      "Calculating vectors for users: 100%|██████████| 1000/1000 [00:03<00:00, 318.41it/s]\n",
      "Calculating probabilities: 100%|██████████| 10989/10989 [00:14<00:00, 758.58it/s]\n",
      "Training:  80%|███████▉  | 1036/1298 [12:03<03:02,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00:12:03, epoch 4, batch 1037\n",
      "validation AUC: 0.6764, MRR: 0.2652, nDCG@5: 0.2676, nDCG@10: 0.3608, ACC: 0.0854\n",
      "\n",
      "┌─────────────┐\n",
      "│4 Epoch Done!│\n",
      "└─────────────┘\n",
      "\n",
      "Epoch 4 Done! Early stop triggered.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Using device:', device)\n",
    "    print(f'Training model {model_name}')\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┐\n",
      "│1 Epoch Done!│\n",
      "└─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print('┌─────────────┐')\n",
    "print('│1 Epoch Done!│')\n",
    "print('└─────────────┘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coongya11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
