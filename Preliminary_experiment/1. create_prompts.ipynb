{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# 사용자 데이터프레임 로드 (예시 파일 경로 사용)\n",
    "# tsv 파일을 DataFrame으로 읽기\n",
    "user = pd.read_csv('data/user.tsv', sep='\\t', names=['User', 'History', 'Train', 'Test'])\n",
    "\n",
    "history_news = pd.read_csv('data/history/news.tsv', sep='\\t',  names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'])\n",
    "train_news = pd.read_csv('data/train/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'])\n",
    "\n",
    "# publish 순서에 맞게 오름차순으로 정렬\n",
    "history_news_sorted = history_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)\n",
    "train_news_sorted = train_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_news(news_sorted, ids, used_ids):\n",
    "    \"\"\"\n",
    "    Negative 샘플로 사용할 뉴스 기사 찾는 함수 (비슷한 발행일 기준)\n",
    "    \"\"\"\n",
    "    negative_train_ids = []\n",
    "    negative_train_titles = []\n",
    "\n",
    "    for id in ids:\n",
    "        # 주어진 News ID에 해당하는 인덱스 찾기\n",
    "        idx = news_sorted[news_sorted['News ID'] == id].index[0]\n",
    "        above_idx, below_idx = idx - 1, idx + 1\n",
    "        similar_ids = []\n",
    "        similar_titles = []\n",
    "\n",
    "        # 이미 존재하는 News ID를 제외하고 위에서 가장 가까운 두 개의 News를 찾기 (이전에 나온 뉴스)\n",
    "        above_count = 0\n",
    "        while above_idx >= 0 and above_count < 2:\n",
    "            if news_sorted.loc[above_idx, 'News ID'] not in used_ids:\n",
    "                similar_ids.append(news_sorted.loc[above_idx, 'News ID'])\n",
    "                similar_titles.append(news_sorted.loc[above_idx, 'Title'])\n",
    "                used_ids.add(news_sorted.loc[above_idx, 'News ID'])\n",
    "                above_count += 1\n",
    "            above_idx -= 1\n",
    "\n",
    "        # 이미 존재하는 News ID를 제외하고 아래에서 가장 가까운 두 개의 News를 찾기 (이후에 나온 뉴스)\n",
    "        below_count = 0\n",
    "        while below_idx < len(news_sorted) and below_count < 2:\n",
    "            if news_sorted.loc[below_idx, 'News ID'] not in used_ids:\n",
    "                similar_ids.append(news_sorted.loc[below_idx, 'News ID'])\n",
    "                similar_titles.append(news_sorted.loc[below_idx, 'Title'])\n",
    "                used_ids.add(news_sorted.loc[below_idx, 'News ID'])\n",
    "                below_count += 1\n",
    "            below_idx += 1\n",
    "\n",
    "        # 필요한 뉴스 수가 4개보다 적을 경우 추가로 위와 아래에서 가져오기\n",
    "        if len(similar_ids) < 4:\n",
    "            remaining_needed = 4 - len(similar_ids)\n",
    "            # 위에서 추가로 가져오기\n",
    "            while above_idx >= 0 and remaining_needed > 0:\n",
    "                if news_sorted.loc[above_idx, 'News ID'] not in used_ids:\n",
    "                    similar_ids.append(news_sorted.loc[above_idx, 'News ID'])\n",
    "                    similar_titles.append(news_sorted.loc[above_idx, 'Title'])\n",
    "                    used_ids.add(news_sorted.loc[above_idx, 'News ID'])\n",
    "                    remaining_needed -= 1\n",
    "                above_idx -= 1\n",
    "\n",
    "            # 아직 부족하면 아래에서 추가로 가져오기\n",
    "            while below_idx < len(news_sorted) and remaining_needed > 0:\n",
    "                if news_sorted.loc[below_idx, 'News ID'] not in used_ids:\n",
    "                    similar_ids.append(news_sorted.loc[below_idx, 'News ID'])\n",
    "                    similar_titles.append(news_sorted.loc[below_idx, 'Title'])\n",
    "                    used_ids.add(news_sorted.loc[below_idx, 'News ID'])\n",
    "                    remaining_needed -= 1\n",
    "                below_idx += 1\n",
    "\n",
    "        # 최종적으로 negative 샘플 리스트에 추가\n",
    "        negative_train_ids.append(similar_ids)\n",
    "        negative_train_titles.append(similar_titles)\n",
    "\n",
    "    return negative_train_ids, negative_train_titles\n",
    "\n",
    "\n",
    "def save_user_file(user_data, output_folder_path, purpose):\n",
    "    \"\"\"\n",
    "    Prompt 저장 함수\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(output_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(output_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)\n",
    "        print(f'[{purpose}] 기존 User Prompts 삭제')    \n",
    "        \n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "    # user prompt를 각 user별 txt파일로 저장\n",
    "    for user_id, content in user_data.items():\n",
    "        file_path = os.path.join(output_folder_path, f\"{user_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "            \n",
    "\n",
    "def save_metadata_file(meta_folder_path, token_counts_and_outputs, user_metadata, purpose):\n",
    "    \"\"\"\n",
    "    metadata 저장 함수\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(meta_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(meta_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)    \n",
    "            \n",
    "    os.makedirs(meta_folder_path, exist_ok=True)\n",
    "\n",
    "    # metadata 저장 위치\n",
    "    meta_file_path = os.path.join(meta_folder_path, \"output_metadata.txt\")\n",
    "\n",
    "    total_input_tokens = 0\n",
    "    total_output_count = 0\n",
    "\n",
    "    # metadata 파일에 user별 token 수, history 수, question 수 등 기록\n",
    "    with open(meta_file_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "        for user_id, token_count, output_count, num_news_ids, num_questions in sorted(token_counts_and_outputs, key=lambda x: int(x[0][1:])):\n",
    "            output_line = (f\"User ID: {user_id:<5} Input Tokens: {token_count:<6} Output Tokens: {output_count:<4}  \"\n",
    "                           f\"History 수: {num_news_ids:<3}  Question 수: {num_questions}\")\n",
    "            total_input_tokens += token_count\n",
    "            total_output_count += output_count\n",
    "\n",
    "            meta_file.write(output_line + \"\\n\")\n",
    "\n",
    "        # 결과 저장\n",
    "        total_line = f\"\\nTotal Input Tokens: {total_input_tokens}\\nTotal Output Tokens: {total_output_count}\"\n",
    "        print(f\"[{purpose}] {total_line}\\n\")\n",
    "        meta_file.write(total_line + \"\\n\")\n",
    "        \n",
    "\n",
    "def save_hidden_positions(meta_folder_path, hidden_positions_data):\n",
    "    \"\"\"\n",
    "    question 정답 저장 함수\n",
    "\n",
    "    \"\"\"\n",
    "    hidden_positions_file_path = os.path.join(meta_folder_path, \"hidden_positions.txt\")\n",
    "    with open(hidden_positions_file_path, \"w\", encoding=\"utf-8\") as hidden_file:\n",
    "        for user_id, positions in hidden_positions_data:\n",
    "            hidden_file.write(f\"{user_id:<5}: {positions}\\n\")\n",
    "            \n",
    "\n",
    "def create(purpose, model, user_count, max_question, save_forder=\"user_prompts\"):\n",
    "    \"\"\"\n",
    "    prompt 생성 main 함수\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # user자별 input과 output 데이터를 저장할 딕셔너리\n",
    "    user_data = {}\n",
    "    user_metadata = []\n",
    "    hidden_positions_data = []\n",
    "\n",
    "    # user ID 목록 생성\n",
    "    user_ids = [f'U{i}' for i in range(1, user_count + 1)]\n",
    "\n",
    "    for ID in user_ids:\n",
    "\n",
    "        # user의 history news title 추출\n",
    "        history = user[user['User'] == ID]['History'].iloc[0]\n",
    "        news_ids = []\n",
    "        [news_ids.append(entry.split(',')[0]) for entry in history.split(';') if entry and entry.split(',')[0] not in news_ids]\n",
    "        titles = []\n",
    "        for news_id in news_ids:\n",
    "            matching_rows = history_news[history_news['News ID'] == news_id]\n",
    "            if not matching_rows.empty:\n",
    "                titles.append(matching_rows.iloc[0]['Title'])\n",
    "\n",
    "        \n",
    "        # user의 train news title 추출\n",
    "        train = user[user['User'] == ID]['Train'].iloc[0]\n",
    "        train_ids = []\n",
    "        [train_ids.append(entry.split(',')[0]) for entry in train.split(';') if entry and entry.split(',')[0] not in train_ids]\n",
    "        train_titles = []\n",
    "        for train_id in train_ids:\n",
    "            matching_rows = train_news[train_news['News ID'] == train_id]\n",
    "            if not matching_rows.empty:\n",
    "                train_titles.append(matching_rows.iloc[0]['Title'])\n",
    "\n",
    "        # History news에 대한 negative news 기사 찾기 (negative용 prompt 생성 시)\n",
    "        if purpose == \"with_negative\":\n",
    "            used_ids = set(news_ids)\n",
    "            negative_ids, negative_titles = find_similar_news(history_news_sorted, news_ids, used_ids)\n",
    "\n",
    "        # Train news에 대한 newgative 기사 찾기\n",
    "        used_ids = set(train_ids)  # 중복을 방지하기 위해 used_ids에 train_id를 추가\n",
    "        negative_train_ids, negative_train_titles = find_similar_news(train_news_sorted, train_ids, used_ids)\n",
    "        \n",
    "        # 질문 생성용 데이터 설정 (Train news와 negative news 결합)\n",
    "        question_ids = [negative_list + [title] for negative_list, title in zip(negative_train_ids, train_ids)]\n",
    "        question_titles = [negative_list + [title] for negative_list, title in zip(negative_train_titles, train_titles)]\n",
    "\n",
    "\n",
    "        # 질문 정답 값의 위치 저장을 위한 리스트 초기화\n",
    "        hidden_positions = []\n",
    "\n",
    "        # 각 행을 섞고 정답 값의 위치를 저장\n",
    "        for row in question_titles:\n",
    "            hidden_value = row[-1]  # 히든 값 (마지막 요소)\n",
    "            random.shuffle(row)     # 전체 행을 섞기\n",
    "            hidden_index = row.index(hidden_value)     # 히든 값의 새로운 위치 찾기\n",
    "            hidden_positions.append(hidden_index + 1)  # 히든 값의 위치 저장\n",
    "\n",
    "        # 정답 위치 데이터 추가\n",
    "        hidden_positions_data.append((ID, hidden_positions))\n",
    "        number = len(titles)\n",
    "\n",
    "        # prompt text 생성\n",
    "        user_content = \"\"\n",
    "        if purpose == \"only_positive\":\n",
    "            user_content += \"[Click History]\\n\"\n",
    "            for i in range(number):\n",
    "                user_content += f\"{i+1}) click : {titles[i]}\\n\"\n",
    "\n",
    "            user_content += \"\\n[Questions]\\nRank the five candidate news for each question based on the user's news interests.\\n\"\n",
    "            for i, titles in enumerate(question_titles[:max_question]):\n",
    "                user_content += f\"Question {i + 1}) \"\n",
    "                user_content += \" / \".join([f\"{j + 1}: {title}\" for j, title in enumerate(titles)]) + \"\\n\"\n",
    "        elif purpose == \"with_negative\":\n",
    "            user_content += \"[News of interest to the user]\\n\"\n",
    "            for i in range(number):\n",
    "                combined_titles = [titles[i]] + negative_titles[i]\n",
    "                random.shuffle(combined_titles)\n",
    "                user_content += f\"{i + 1}) \" + \" / \".join(combined_titles) + \"\\n\"\n",
    "                user_content += f\"Of the five news above, the news that the user is most interested in : {titles[i]}\\n\\n\"\n",
    "\n",
    "            user_content += \"[Questions]\\nRank the five candidate news for each question based on the user's news interests.\\n\"\n",
    "            for i, titles in enumerate(question_titles[:max_question]):\n",
    "                user_content += f\"Question {i + 1}) \"\n",
    "                user_content += \" / \".join([f\"{j + 1}: {title}\" for j, title in enumerate(titles)]) + \"\\n\"\n",
    "\n",
    "        user_content += \"\\nDon't explain why in your answer, just list news articles ranked for each question.\\n\"\n",
    "        \n",
    "        # user prompt를 딕셔너리에 저장\n",
    "        user_data[ID] = user_content\n",
    "        user_metadata.append((ID, number, len(question_titles)))\n",
    "\n",
    "    # user prompt 파일 저장\n",
    "    output_folder_path = f\"{save_forder}/{purpose}\"\n",
    "    save_user_file(user_data, output_folder_path, purpose)\n",
    "\n",
    "    # 각 user별 token 수와 output 값을 저장할 리스트\n",
    "    token_counts_and_outputs = []\n",
    "\n",
    "    # tiktoken을 사용하여 토큰 수 계산\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    # 모든 텍스트 파일에 대해 반복 수행\n",
    "    for file_path in sorted(glob.glob(os.path.join(f\"{save_forder}/{purpose}\", \"*.txt\"))):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            # 파일에서 input 텍스트 추출\n",
    "            input_text = file.read()\n",
    "\n",
    "            # 토큰 수 계산\n",
    "            token_count = len(encoding.encode(input_text))\n",
    "\n",
    "            # 사용자 ID 추출\n",
    "            user_id = os.path.basename(file_path).split(\".\")[0]\n",
    "\n",
    "            # Question 수에 따른 output count 계산\n",
    "            num_questions = input_text.count(\"Question \")\n",
    "            output_count = 18 + (num_questions - 1) * 19 if num_questions > 0 else 0\n",
    "\n",
    "            # 사용자 메타데이터에서 number와 num_questions 가져오기\n",
    "            # number = next((item[1] for item in user_metadata if item[0] == user_id),0)\n",
    "            number = next(item[1] for item in user_metadata if item[0] == user_id)\n",
    "\n",
    "            # 사용자 ID, 토큰 수, output count, news_ids 수, question 수를 리스트에 추가\n",
    "            token_counts_and_outputs.append((user_id, token_count, output_count, number, num_questions))\n",
    "\n",
    "    \n",
    "    # metadata 파일 저장\n",
    "    meta_folder_path = f\"{save_forder}/{purpose}/metadata\"\n",
    "    save_metadata_file(meta_folder_path, token_counts_and_outputs, user_metadata, purpose)\n",
    "\n",
    "    # question 정답 위치 저장\n",
    "    save_hidden_positions(meta_folder_path, hidden_positions_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행\n",
    "[create 함수]\n",
    "- purpose = 어떤 목적으로 prompt를 생성할 것인지  [only_positive / with_negative]   \n",
    "- model = 사용할 gpt (token 계산 용도)  [gpt-40-mini / gpt-3.5-turbo]\n",
    "- user_count = 몇 명의 user prompt를 생성할 것인지\n",
    "- max_question = 최대 질문 수\n",
    "- save_forder = 결과를 저장할 폴더 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[only_positive] 기존 User Prompts 삭제\n",
      "[only_positive] \n",
      "Total Input Tokens: 2233365\n",
      "Total Output Tokens: 364003\n",
      "\n",
      "[with_negative] 기존 User Prompts 삭제\n",
      "[with_negative] \n",
      "Total Input Tokens: 4263281\n",
      "Total Output Tokens: 364003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create(purpose='only_positive', model=\"gpt-4o-mini\", user_count=1500, max_question=30, save_forder=\"user_prompts\")\n",
    "create(purpose='with_negative', model=\"gpt-4o-mini\", user_count=1500, max_question=30, save_forder=\"user_prompts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coongya11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
