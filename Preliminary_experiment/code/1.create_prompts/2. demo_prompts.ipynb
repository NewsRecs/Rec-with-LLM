{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# user DF에서 History column과 Train column을 합쳐서 history로 사용\n",
    "user = pd.read_csv('../../data/user.tsv', sep='\\t', names=['User', 'History', 'Train', 'Question'])\n",
    "user['History'] = user['History'] + user['Train']\n",
    "user = user.drop(columns=['Train'])\n",
    "\n",
    "history_news = pd.read_csv('../../data/history/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "train_news = pd.read_csv('../../data/train/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "history_news = pd.concat([history_news, train_news], ignore_index=True)\n",
    "history_news[['Category_New', 'SubCategory']] = history_news['Category'].str.split('|', expand=True)\n",
    "history_news = history_news.drop('Category', axis=1).rename(columns={'Category_New': 'Category'})\n",
    "\n",
    "question_news = pd.read_csv('../../data/test/news.tsv', sep='\\t', names=['News ID', 'Publish', 'Title', 'Click time history', 'Category'], parse_dates=['Publish'])\n",
    "question_news[['Category_New', 'SubCategory']] = question_news['Category'].str.split('|', expand=True)\n",
    "question_news = question_news.drop('Category', axis=1).rename(columns={'Category_New': 'Category'})\n",
    "\n",
    "\n",
    "# publish 순서에 맞게 오름차순으로 정렬\n",
    "history_news_sorted = history_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)\n",
    "question_news_sorted = question_news.sort_values(by='Publish', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "The news articles that User #2074 clicked before are as follows:\n",
    "1. Se lesernes nyttårsbilder\n",
    "2. Politiet frigir navn på kvinne som døde etter E39-ulykke\n",
    "3. Nå er navnet frigitt etter dødsfall i Gjemnes\n",
    "4. Dette har Rema 1000 søkt om å bruke Æ-varemerket til\n",
    "5. - Helt ufattelig at det går an å behandle en ny bil på denne måten\n",
    "6. Slik gikk det da elbilene måtte betale p-avgift\n",
    "7. Boligselger må punge ut for å ha krysset av feil om oppussing\n",
    "8. Tøft Rema-år er bakteppe for «Æ»\n",
    "9. Reitan etablerer ny bensinstasjonkjede\n",
    "10. Oppretter ikke straffesak etter denne utforkjøringen\n",
    "11. Trafikkuhell i Elgesetergate\n",
    "12. To vogntog har kollidert\n",
    "13. Nå har Rema avslørt hva Æ er\n",
    "14. Æ-en ble en nedtur\n",
    "15. Koteng mener Reitan dekkes for negativt\n",
    "16. - Vi skal ikke drive med biljakt\n",
    "17. Nils Arne Eggen: - Akkurat nå bruker jeg gåstol og krykker\n",
    "18. Ønsket seg utedo-feeling på badet\n",
    "\n",
    "Based on the articles the user clicked before, User #2074 prefers most [MASK] among the following five articles:\n",
    "1: Politiet har tre ulike teorier etter dødsfallet i Tydal\n",
    "2: Flyktninger frøs i hjel i Bulgaria\n",
    "3: Personbil krasjet i trailer etter forbikjøring\n",
    "4: Nå skal disse bort fra Londons gater\n",
    "5: Eksperter gir råd om Norges bidrag til NATOs rakettforsvar\n",
    "Question 1. The index number of the [MASK] is 3\n",
    "\n",
    "Based on the articles the user clicked before, User #2074 prefers most [MASK] among the following five articles:\n",
    "1: Gigantisk isfjell holder på å brekke av i Antarktis\n",
    "2: Nå endrer vi matvanene. Det får ekspertene til å juble\n",
    "3: Uklart om flyplasskyting i Florida var terrorangrep\n",
    "4: Selvskrytet ville nesten ingen ende ta. Trondhjemmerne var i skyene\n",
    "5: Hva er vitsen med å bo i Norge hvis man hater snø?\n",
    "Question 2. The index number of the [MASK] is 2\n",
    "\n",
    "The news articles that User #2079 clicked before are as follows:\n",
    "1. Se lesernes nyttårsbilder\n",
    "2. Her koker det over for Tønseth. Så stakk han fra stadion i sinne.\n",
    "3. Åpenbart beruset mann i trafikkulykke på Byåsen\n",
    "4. Frontkollisjon på Heimdal\n",
    "5. Slik bor Trondheims «Anno»-deltagere\n",
    "6. To skadde etter dødsulykke på E39 får behandling ved St. Olav\n",
    "7. Dette har Rema 1000 søkt om å bruke Æ-varemerket til\n",
    "8. Politiet frigir navn på kvinne som døde etter E39-ulykke\n",
    "9. Nå er navnet frigitt etter dødsfall i Gjemnes\n",
    "10. - Helt ufattelig at det går an å behandle en ny bil på denne måten\n",
    "11. - Dette er ikke Senterpartiets olje- og gasspolitikk\n",
    "12. - Fast ansatte er et konkurransefortrinn\n",
    "13. NHO vil fjerne mastergrader\n",
    "14. Gikk du glipp av romjulssalget? Frykt ikke, tidenes januarsalg er her. Igjen\n",
    "15. Da han kom tilbake fra alpinbakken så bilen slik ut\n",
    "16. To vogntog har kollidert\n",
    "17. Det får da være grenser for dårlig planlegging ved kjøp av nye togvogner\n",
    "18. Æ-en ble en nedtur\n",
    "19. En usedvanlig imponerende smartmobil\n",
    "20. Trafikkuhell i Elgesetergate\n",
    "21. Her er Samsungs nye super-TV-er\n",
    "22. Lastebil mistet last med metallrør i veien\n",
    "23. Æ-appen førte til betalingsproblemer i Rema-butikkene\n",
    "24. Almhjell forlater sitt fantasy-univers\n",
    "25. Stor test av 42 typer knekkebrød\n",
    "26. Dette var starten for avisen som er eldre enn USA\n",
    "27. Norsk Tipping deler ut 448 millioner. Se hvor mye din klubb får.\n",
    "28. Historiske trondheimsbilder: Her tar Tyholttårnet form\n",
    "29. En telefon uten knapper? Vi trodde knapt det var mulig\n",
    "30. Kødannelse på E6 etter at bil mistet hjul\n",
    "31. Nils Arne Eggen: - Akkurat nå bruker jeg gåstol og krykker\n",
    "\n",
    "Based on the articles the user clicked before, User #2079 prefers most [MASK] among the following five articles:\n",
    "1: Tande har funnet årsaken til fiaskohoppet: Tar på seg all skyld\n",
    "2: Tror de lave rentene er blitt en sovepute for naive lånekunder\n",
    "3: Tankestreif: - Jeg er da ikke helt idiot heller!\n",
    "4: VM i Lahti kan gå uten en eneste Northug: – Akkurat nå ser det mørkt ut\n",
    "5: 3T åpner to nye treningssenter\n",
    "Question 1. The index number of the [MASK] is 5\n",
    "\n",
    "Based on the articles the user clicked before, User #2079 prefers most [MASK] among the following five articles:\n",
    "1: Gigantisk isfjell holder på å brekke av i Antarktis\n",
    "2: Nå skal disse bort fra Londons gater\n",
    "3: Svensk rapport: Russland forsøker å spre falske nyheter\n",
    "4: - Det ser ikke ut som om det har skjedd noe straffbart\n",
    "5: Politiet har tre ulike teorier etter dødsfallet i Tydal\n",
    "Question 2. The index number of the [MASK] is 1\n",
    "\n",
    "Based on the articles the user clicked before, User #2079 prefers most [MASK] among the following five articles:\n",
    "1: Weng og Østberg parkert etter smørebom: – Dette var grusomt\n",
    "2: Personbil krasjet i trailer etter forbikjøring\n",
    "3: Selvskrytet ville nesten ingen ende ta. Trondhjemmerne var i skyene\n",
    "4: Nå endrer vi matvanene. Det får ekspertene til å juble\n",
    "5: Kantspiller klar for Rosenborg: – En god løsning\n",
    "Question 3. The index number of the [MASK] is 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_ids_and_click_times(news_str):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 뉴스 click history 문자열에서 뉴스 ID와 클릭 시간을 추출하여 각각의 리스트로 반환 (중복된 뉴스 ID는 제외)\n",
    "\n",
    "    설명:\n",
    "    - <news_str>을 세미콜론으로 분리한 후, 각 항목에서 뉴스 ID와 클릭 시간을 추출\n",
    "    - 중복된 뉴스 ID는 <seen_ids> 집합을 사용하여 제외\n",
    "    - 클릭 시간은 pandas의 <to_datetime> 함수를 사용하여 datetime 객체로 변환됨\n",
    "    \"\"\"\n",
    "    news_ids = []     # 뉴스 ID를 저장할 리스트\n",
    "    click_times = []  # 클릭 시간을 저장할 리스트\n",
    "    seen_ids = set()  # 중복된 뉴스 ID를 추적하기 위한 집합\n",
    "    entries = news_str.strip(';').split(';')  # 세미콜론으로 구분된 항목들 분리\n",
    "    for entry in entries:\n",
    "        if entry:\n",
    "            parts = entry.split(',')  # 쉼표로 구분된 부분들 분리\n",
    "            if len(parts) >= 3:\n",
    "                news_id = parts[0]  # 첫 번째 부분을 뉴스 ID로 사용\n",
    "                click_time_str = parts[-1]  # 마지막 부분을 클릭 시간으로 사용\n",
    "                click_time = pd.to_datetime(click_time_str)  # 문자열을 datetime으로 변환\n",
    "                if news_id not in seen_ids:\n",
    "                    seen_ids.add(news_id)  # 중복 방지를 위해 집합에 추가\n",
    "                    news_ids.append(news_id)\n",
    "                    click_times.append(click_time)\n",
    "    return news_ids, click_times\n",
    "\n",
    "def find_similar_news(news_df, news_ids, click_times, used_ids, used_titles, low_freq_categories):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문 뉴스와 유사한 뉴스 기사들 중 'low_freq_categories'에 해당하는 부정적 샘플을 찾아 반환\n",
    "    - 각 질문 뉴스에 대해 최대 4개의 부정적 샘플을 선택\n",
    "\n",
    "    변경 사항:\n",
    "    - 사용자 history category 빈도를 기준으로 low_freq_categories를 필터링 조건으로 추가\n",
    "    - 24시간 이내 뉴스 -> 조건 충족 안되면 더 과거의 뉴스\n",
    "    \"\"\"\n",
    "    negative_ids = []\n",
    "    negative_titles = []\n",
    "\n",
    "    for news_id, click_time in zip(news_ids, click_times):\n",
    "        time_window_start = click_time - pd.Timedelta(hours=24)\n",
    "\n",
    "        # 1. 24시간 이내의 negative sample 후보 찾기\n",
    "        candidate_news = news_df[\n",
    "            (news_df['Publish'] >= time_window_start) & \n",
    "            (news_df['Publish'] <= click_time) & \n",
    "            (~news_df['News ID'].isin(used_ids)) & \n",
    "            (~news_df['Title'].isin(used_titles)) & \n",
    "            (news_df['SubCategory'].isin(low_freq_categories))\n",
    "        ]\n",
    "\n",
    "        # 제목 중복 제거\n",
    "        candidate_news = candidate_news.drop_duplicates(subset='Title')\n",
    "\n",
    "        needed = 4\n",
    "        selected_news_list = []\n",
    "\n",
    "        if len(candidate_news) >= needed:\n",
    "            # 24시간 이내에서 충분히 구할 수 있는 경우\n",
    "            selected_news = candidate_news.sample(n=needed, random_state=42)\n",
    "            selected_news_list.append(selected_news)\n",
    "        else:\n",
    "            # 24시간 이내에서 부족한 경우\n",
    "            selected_news_list.append(candidate_news)\n",
    "            needed -= len(candidate_news)\n",
    "\n",
    "            # 2. 더 과거에서 추가 검색 (가장 최신 순)\n",
    "            extra_news = news_df[\n",
    "                (news_df['Publish'] < time_window_start) &\n",
    "                (~news_df['News ID'].isin(used_ids)) &\n",
    "                (~news_df['Title'].isin(used_titles)) &\n",
    "                (news_df['SubCategory'].isin(low_freq_categories))\n",
    "            ]\n",
    "            extra_news = extra_news.drop_duplicates(subset='Title')\n",
    "\n",
    "            # time_window_start와의 시간 차이를 계산하여 가장 가까운 뉴스부터 선택\n",
    "            extra_news = extra_news.assign(TimeDiff=(time_window_start - extra_news['Publish']).abs())\n",
    "            extra_news = extra_news.sort_values(by='TimeDiff')\n",
    "\n",
    "            extra_selected_news = extra_news.head(needed)\n",
    "            selected_news_list.append(extra_selected_news)\n",
    "\n",
    "        # 선택된 뉴스들 합치기\n",
    "        selected_news = pd.concat(selected_news_list)\n",
    "\n",
    "        # 최종적으로 제목 중복 제거 및 섞기\n",
    "        selected_news = selected_news.drop_duplicates(subset='Title').sample(frac=1, random_state=42)\n",
    "\n",
    "        # negative sample을 최대 4개만 사용\n",
    "        selected_news = selected_news.head(4)\n",
    "\n",
    "        similar_ids = selected_news['News ID'].tolist()\n",
    "        similar_titles = selected_news['Title'].tolist()\n",
    "\n",
    "        negative_ids.append(similar_ids)\n",
    "        negative_titles.append(similar_titles)\n",
    "\n",
    "        # 사용한 ID, Title 업데이트\n",
    "        used_ids.update(similar_ids)\n",
    "        used_titles.update(similar_titles)\n",
    "\n",
    "    return negative_ids, negative_titles\n",
    "\n",
    "def find_similar_question_news(news_df, news_ids, click_times, used_ids, used_titles):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문 뉴스와 유사한 뉴스 기사들 중 부정적 샘플을 찾아 반환\n",
    "    - 각 질문 뉴스에 대해 최대 4개의 부정적 샘플을 선택\n",
    "\n",
    "    설명:\n",
    "    - 각 질문 뉴스에 대해 클릭 시간 기준으로 24시간 이내에 발행된 뉴스 중에서 사용되지 않은 뉴스 ID와 제목을 가진 후보들을 찾음\n",
    "    - 후보들 중에서 중복된 제목을 제거한 후, 필요한 수만큼 랜덤하게 샘플링\n",
    "    - 필요한 수만큼의 샘플이 부족할 경우, 더 과거의 뉴스에서 추가로 샘플을 선택\n",
    "    - 선택된 뉴스 ID와 제목을 리스트에 저장하고, 이후 중복 방지를 위해 used_ids와 used_titles를 업데이트\n",
    "    \"\"\"\n",
    "    negative_ids = []  # negative sample의 뉴스 ID 리스트\n",
    "    negative_titles = []  # negative sample의 뉴스 제목 리스트\n",
    "\n",
    "    for id, click_time in zip(news_ids, click_times):\n",
    "        time_window_start = click_time - pd.Timedelta(hours=24)\n",
    "        candidate_news = news_df[\n",
    "            (news_df['Publish'] >= time_window_start) & \n",
    "            (news_df['Publish'] <= click_time) & \n",
    "            (~news_df['News ID'].isin(used_ids)) & \n",
    "            (~news_df['Title'].isin(used_titles))\n",
    "        ]\n",
    "\n",
    "        # 제목별로 중복 제거\n",
    "        candidate_news = candidate_news.drop_duplicates(subset='Title')\n",
    "\n",
    "        # 필요한 negative sample 수\n",
    "        needed = 4\n",
    "        selected_news_list = []\n",
    "\n",
    "        if len(candidate_news) >= needed:\n",
    "            selected_news = candidate_news.sample(n=needed, random_state=42)  # 랜덤 sampling\n",
    "            selected_news_list.append(selected_news)\n",
    "        else:\n",
    "            selected_news_list.append(candidate_news)\n",
    "            needed -= len(candidate_news)\n",
    "\n",
    "            # 추가로 필요한 negative sample을 더 먼 과거에서 가져오기\n",
    "            extra_news = news_df[\n",
    "                (news_df['Publish'] < time_window_start) &\n",
    "                (~news_df['News ID'].isin(used_ids)) &\n",
    "                (~news_df['Title'].isin(used_titles))\n",
    "            ]\n",
    "            extra_news = extra_news.drop_duplicates(subset='Title')\n",
    "            extra_news = extra_news.assign(TimeDiff=(time_window_start - extra_news['Publish']).abs())\n",
    "            extra_news = extra_news.sort_values(by='TimeDiff')\n",
    "            extra_selected_news = extra_news.head(needed)\n",
    "            selected_news_list.append(extra_selected_news)\n",
    "\n",
    "        # 선택된 뉴스들을 하나의 DataFrame으로 결합\n",
    "        selected_news = pd.concat(selected_news_list)\n",
    "\n",
    "        # 최종적으로 제목 중복 제거 및 섞기\n",
    "        selected_news = selected_news.drop_duplicates(subset='Title')\n",
    "        selected_news = selected_news.sample(frac=1, random_state=42)\n",
    "\n",
    "        # 만약 negative sample 수가 부족하면 가능한 만큼만 사용\n",
    "        selected_news = selected_news.head(4)\n",
    "\n",
    "        similar_ids = selected_news['News ID'].tolist()\n",
    "        similar_titles = selected_news['Title'].tolist()\n",
    "\n",
    "        negative_ids.append(similar_ids)\n",
    "        negative_titles.append(similar_titles)\n",
    "\n",
    "        # 사용된 ID, Title 업데이트\n",
    "        used_ids.update(similar_ids)\n",
    "        used_titles.update(similar_titles)\n",
    "\n",
    "    return negative_ids, negative_titles\n",
    "\n",
    "def save_user_file(user_data, output_folder_path, purpose):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 사용자별 prompt data를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 지정된 output_folder_path 경로에 기존의 .txt 파일들을 삭제\n",
    "    - 각 사용자에 대해 user_id를 파일명으로 하는 .txt 파일을 생성하고, 해당 사용자 데이터를 저장\n",
    "    \"\"\"\n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(output_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(output_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)\n",
    "        print(f'[{purpose}] 기존 User Prompts 삭제')    \n",
    "\n",
    "    os.makedirs(output_folder_path, exist_ok=True)  # 폴더 생성\n",
    "\n",
    "    # user prompt를 각 user별 txt파일로 저장\n",
    "    for user_id, content in user_data.items():\n",
    "        file_path = os.path.join(output_folder_path, f\"{user_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "\n",
    "def save_metadata_file(meta_folder_path, token_counts_and_outputs, user_metadata, purpose):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 메타데이터(토큰 수, 출력 수 등)를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 지정된 meta_folder_path 경로에 기존의 .txt 파일들을 삭제\n",
    "    - 각 사용자에 대해 토큰 수, 출력 수, 히스토리 수, 질문 수 등을 기록하여 output_metadata.txt 파일에 저장\n",
    "    - 총 입력 토큰 수와 출력 토큰 수도 함께 기록\n",
    "    \"\"\"\n",
    "    # 기존 파일들 삭제\n",
    "    if os.path.exists(meta_folder_path):\n",
    "        for file_path in glob.glob(os.path.join(meta_folder_path, \"*.txt\")):\n",
    "            os.remove(file_path)    \n",
    "\n",
    "    os.makedirs(meta_folder_path, exist_ok=True)    # 폴더 생성\n",
    "\n",
    "    # metadata 저장 위치\n",
    "    meta_file_path = os.path.join(meta_folder_path, \"output_metadata.txt\")\n",
    "\n",
    "    total_input_tokens = 0  # 총 입력 토큰 수\n",
    "    total_output_count = 0  # 총 출력 토큰 수\n",
    "\n",
    "    # user별 metadata(token 수, history 수, question 수 등) 기록\n",
    "    with open(meta_file_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "        for user_id, token_count, output_count, num_news_ids, num_questions in sorted(token_counts_and_outputs, key=lambda x: int(x[0][1:])):\n",
    "            output_line = (f\"User ID: {user_id:<5} Input Tokens: {token_count:<6} Output Tokens: {output_count:<4}  \"\n",
    "                           f\"History 수: {num_news_ids:<3}  Question 수: {num_questions}\")\n",
    "            total_input_tokens += token_count\n",
    "            total_output_count += output_count\n",
    "\n",
    "            meta_file.write(output_line + \"\\n\")\n",
    "\n",
    "        # 총 합계 기록\n",
    "        total_line = f\"\\nTotal Input Tokens: {total_input_tokens}\\nTotal Output Tokens: {total_output_count}\"\n",
    "        print(f\"[{purpose}] {total_line}\\n\")\n",
    "        meta_file.write(total_line + \"\\n\")\n",
    "\n",
    "def save_hidden_positions(meta_folder_path, hidden_positions_data):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 질문의 정답 위치(인덱스)를 텍스트 파일로 저장\n",
    "\n",
    "    설명:\n",
    "    - 각 사용자에 대해 정답 위치를 기록하여 hidden_positions.txt 파일에 저장\n",
    "    \"\"\"\n",
    "    hidden_positions_file_path = os.path.join(meta_folder_path, \"hidden_positions.txt\")\n",
    "    with open(hidden_positions_file_path, \"w\", encoding=\"utf-8\") as hidden_file:\n",
    "        for user_id, positions in hidden_positions_data:\n",
    "            hidden_file.write(f\"{user_id:<5}: {positions}\\n\")\n",
    "\n",
    "def generate_demonstrations(user_name, history_titles, history_negative_titles):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - Demo용 문장을 생성하여 사용자 프롬프트에 추가\n",
    "\n",
    "    설명:\n",
    "    - 사용자의 히스토리 제목과 부정적 샘플 제목을 기반으로 데모 문장을 생성\n",
    "    - 실제 정답의 위치를 [MASK]로 표시하고, 후보 제목들을 무작위로 섞어 표시\n",
    "    \"\"\"\n",
    "    demonstration_lines = []\n",
    "    # demonstration_lines.append(f\"Here are some demonstrations intended for {user_name}.\\n\")\n",
    "\n",
    "    for i, (h_title, neg_titles) in enumerate(zip(history_titles, history_negative_titles)):\n",
    "        candidates = neg_titles + [h_title]\n",
    "        random.shuffle(candidates)  # 후보 제목들 섞기\n",
    "        index_of_actual = candidates.index(h_title) + 1  # 실제 정답의 인덱스 (1부터 시작)\n",
    "\n",
    "        demonstration_lines.append(f\"{user_name} prefers most [MASK] among the following five articles:\")\n",
    "        for j, candidate_title in enumerate(candidates):\n",
    "            demonstration_lines.append(f\"{j+1}: {candidate_title}\")\n",
    "        demonstration_lines.append(f\"The index number of the [MASK] is {index_of_actual}.\\n\")\n",
    "\n",
    "    return demonstration_lines\n",
    "\n",
    "def create_prompts(purpose, model, user_count, max_question, max_history=300, save_folder=\"user_prompts\", start_count=0):\n",
    "    \"\"\"\n",
    "    목적: \n",
    "    - 전체 프로세스를 통해 사용자별 프롬프트를 생성하고, 관련 파일들을 저장\n",
    "\n",
    "    설명:\n",
    "    - 사용자 데이터를 순회하면서 히스토리 뉴스와 질문 뉴스의 ID 및 클릭 시간을 추출\n",
    "    - positive 및 negative sample을 찾아 프롬프트를 생성\n",
    "    - 생성된 프롬프트와 metadata를 파일로 저장\n",
    "    - purpose에 따라 프롬프트의 내용이 달라지며, with_negative인 경우 부정적 샘플을 포함\n",
    "    \"\"\"\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    formatted_date = now.strftime(\"%y%m%d\")\n",
    "\n",
    "    save_folder = f'../../prompts/[{formatted_date}-2] {save_folder}'\n",
    "\n",
    "    # 사용자별 데이터 초기화\n",
    "    user_data = {}\n",
    "    token_counts_and_outputs = []\n",
    "    hidden_positions_data = []\n",
    "\n",
    "    # 히스토리의 최대 길이 설정\n",
    "    max_history = int(f'-{max_history}')\n",
    "\n",
    "    # tiktoken을 사용하여 토큰 수 계산\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    # 사용자 목록 생성 (데이터프레임 'user'의 상위 user_count만 선택)\n",
    "    selected_users = user.iloc[start_count:user_count]\n",
    "\n",
    "    for idx, row in selected_users.iterrows():\n",
    "        user_id = row['User']\n",
    "        user_name = f\"User #{user_id[0:]}\"  # 사용자 이름 형식 지정\n",
    "\n",
    "        # history 뉴스 ID 및 클릭 시간 추출 (중복 제거됨)\n",
    "        history_str = row['History']\n",
    "        history_news_ids, history_click_times = extract_news_ids_and_click_times(history_str)\n",
    "        history_news_ids = history_news_ids[max_history:]\n",
    "        history_click_times = history_click_times[max_history:]\n",
    "\n",
    "        # history 뉴스 Title 추출\n",
    "        history_titles = []\n",
    "        for news_id in history_news_ids:\n",
    "            matching_rows = history_news[history_news['News ID'] == news_id]\n",
    "            if not matching_rows.empty:\n",
    "                title = matching_rows.iloc[0]['Title']\n",
    "                history_titles.append(title)\n",
    "\n",
    "        # question 뉴스 ID 및 클릭 시간 추출 (중복 제거됨)\n",
    "        question_str = row['Question']\n",
    "        question_news_ids, question_click_times = extract_news_ids_and_click_times(question_str)\n",
    "\n",
    "        # question 뉴스 제목 추출\n",
    "        question_titles_list = []\n",
    "        for news_id in question_news_ids:\n",
    "            matching_rows = question_news[question_news['News ID'] == news_id]\n",
    "            if not matching_rows.empty:\n",
    "                title = matching_rows.iloc[0]['Title']\n",
    "                question_titles_list.append(title)\n",
    "                \n",
    "                \n",
    "\n",
    "        # used_ids와 used_titles 초기화 (이미 사용된 ID와 제목)\n",
    "        used_ids = set(history_news_ids + question_news_ids)\n",
    "        used_titles = set(history_titles + question_titles_list)\n",
    "        \n",
    "\n",
    "        user_history_categories = history_news[history_news['News ID'].isin(history_news_ids)]['SubCategory']\n",
    "        category_counts = Counter(user_history_categories)\n",
    "        all_categories = set(history_news['SubCategory'].unique()).union(question_news['SubCategory'].unique())\n",
    "        low_freq_categories = {cat for cat in all_categories if category_counts[cat] <= 0}\n",
    "\n",
    "        # history 뉴스에 대한 negative 샘플 찾기\n",
    "        if purpose == 'with_negative':\n",
    "            history_negative_ids, history_negative_titles = find_similar_news(\n",
    "                history_news, history_news_ids, history_click_times, used_ids, used_titles, low_freq_categories\n",
    "            )\n",
    "\n",
    "            # 데모 생성\n",
    "            demonstration_lines = generate_demonstrations(\n",
    "                user_name, history_titles, history_negative_titles\n",
    "            )\n",
    "        else:\n",
    "            demonstration_lines = []\n",
    "            demonstration_lines.append(\"Here are some demonstrations\")\n",
    "            demonstration_lines.append(instruction)\n",
    "\n",
    "        # question 뉴스에 대한 negative sample 찾기\n",
    "        negative_question_ids, negative_question_titles = find_similar_question_news(\n",
    "            question_news, question_news_ids, question_click_times, used_ids, used_titles\n",
    "        )\n",
    "\n",
    "        # 각 question에 대한 candidate title 준비\n",
    "        all_candidate_titles = []\n",
    "        hidden_positions = []\n",
    "\n",
    "        for q_title, neg_titles in zip(question_titles_list, negative_question_titles):\n",
    "            \n",
    "            neg_titles = [title for title in neg_titles if title != q_title]    # neg_titles에서 q_title과 동일한 제목을 제거\n",
    "            neg_titles = list(dict.fromkeys(neg_titles))    # neg_titles에서 제목의 중복 제거\n",
    "\n",
    "            # negative 샘플이 4개 미만일 경우 처리  (현재는 반복을 중단)\n",
    "            while len(neg_titles) < 4:\n",
    "                break  # 일단 반복을 중단하고 현재 상태로 진행\n",
    "\n",
    "            # 최대 4개의 negative 샘플과 q_title을 후보로 사용\n",
    "            candidates = neg_titles[:4] + [q_title]\n",
    "            # 후보에서 제목의 중복 제거\n",
    "            candidates = list(dict.fromkeys(candidates))\n",
    "\n",
    "            # 후보가 5개가 되도록 보장 (필요 시 추가 로직 구현 가능)\n",
    "            if len(candidates) < 5:\n",
    "                print(f'idx, question수 5개 미만')\n",
    "                pass\n",
    "\n",
    "            random.shuffle(candidates)  # 후보 섞기\n",
    "            all_candidate_titles.append(candidates)\n",
    "            index_of_actual = candidates.index(q_title) + 1  # 실제 정답의 인덱스 (1부터 시작)\n",
    "            hidden_positions.append(index_of_actual)\n",
    "\n",
    "\n",
    "        # 프롬프트 텍스트 생성\n",
    "        prompt_lines = []\n",
    "\n",
    "        # 데모 부분 추가\n",
    "        prompt_lines.extend(demonstration_lines)\n",
    "\n",
    "        # 사용자의 클릭 히스토리 추가\n",
    "        if purpose == 'only_positive':\n",
    "            prompt_lines.append(f\"\\nThe news articles that {user_name} clicked before are as follows:\")\n",
    "            for i, title in enumerate(history_titles):\n",
    "                prompt_lines.append(f\"{i+1}. {title}\")\n",
    "            prompt_lines.append(\"\\n\")\n",
    "\n",
    "        # question 부분 추가\n",
    "        if purpose == 'with_negative':\n",
    "            # prompt_lines.append(f\"Based on these demonstrations for {user_name}, predict the index number of news article that best fits in the position labeled [MASK] for each following question in terms of {user_name}.\\n\")\n",
    "            prompt_lines.append(f\"Based on {user_name}'s preferences, predict the index number of the news article that best fits the position labeled [MASK] for each question.\\n\")\n",
    "        # else:\n",
    "        #     prompt_lines.append(f\"Based on the articles the user clicked before, {user_name} prefers most [MASK] among the following five articles:\")\n",
    "\n",
    "        for i, candidates in enumerate(all_candidate_titles[:max_question]):\n",
    "            if purpose == 'with_negative':\n",
    "                prompt_lines.append(f\"{user_name} prefers most [MASK] among the following five articles:\")\n",
    "            else:\n",
    "                prompt_lines.append(f\"Based on the articles the user clicked before, {user_name} prefers most [MASK] among the following five articles:\")\n",
    "            for j, candidate_title in enumerate(candidates):\n",
    "                prompt_lines.append(f\"{j+1}: {candidate_title}\")\n",
    "            prompt_lines.append(f\"Question {i+1}. The index number of the [MASK] is ?\\n\")\n",
    "            num_questions = i+1\n",
    "\n",
    "        prompt_lines.append(f\"Please provide just the answers to each of {user_name}'s question without any explanations.\")\n",
    "        prompt_text = '\\n'.join(prompt_lines)\n",
    "        \n",
    "        user_data[user_id] = prompt_text\n",
    "        hidden_positions_data.append((user_id, hidden_positions))\n",
    "        token_count = len(encoding.encode(prompt_text))  # 토큰 수 계산\n",
    "        output_count = num_questions  # 출력 수\n",
    "        num_history_titles = len(history_titles)  # 히스토리 제목 수\n",
    "        token_counts_and_outputs.append((user_id, token_count, output_count, num_history_titles, num_questions))\n",
    "\n",
    "    # prompt 및 metadata 저장\n",
    "    output_folder_path = os.path.join(save_folder, purpose)\n",
    "    save_user_file(user_data, output_folder_path, purpose)\n",
    "\n",
    "    # metadata 파일 저장\n",
    "    meta_folder_path = os.path.join(save_folder, purpose, 'metadata')\n",
    "    save_metadata_file(meta_folder_path, token_counts_and_outputs, None, purpose)\n",
    "    save_hidden_positions(meta_folder_path, hidden_positions_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[with_negative] \n",
      "Total Input Tokens: 226283\n",
      "Total Output Tokens: 280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_prompts(purpose='only_positive',\n",
    "               model=\"gpt-4o-mini\",\n",
    "               user_count=1000,\n",
    "               max_question=15,\n",
    "               max_history=60,\n",
    "               save_folder=\"임시용2\",\n",
    "               start_count=0\n",
    "               )\n",
    "\n",
    "create_prompts(purpose='with_negative',\n",
    "               model=\"gpt-4o-mini\",\n",
    "               user_count=1000,\n",
    "               max_question=15,\n",
    "               max_history=60,\n",
    "               save_folder=\"임시용2\",\n",
    "               start_count=0\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>History</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U1</td>\n",
       "      <td>N1 N1 N317 N329 N521 N533 N874 N830 N868 N933 ...</td>\n",
       "      <td>N4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2</td>\n",
       "      <td>N1 N1 N3 N98 N1 N329 N15 N385 N385 N817 N831 N...</td>\n",
       "      <td>N3589 N3553 N3987 N4395 N4453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U3</td>\n",
       "      <td>N1 N1 N387 N385 N329 N335 N496 N868 N1057 N830...</td>\n",
       "      <td>N4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U4</td>\n",
       "      <td>N2 N2 N148 N206 N148 N335 N329 N408 N868 N879 ...</td>\n",
       "      <td>N4475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U5</td>\n",
       "      <td>N3 N3 N3 N230 N335 N329 N329 N426 N408 N432 N7...</td>\n",
       "      <td>N3539 N3550 N3550 N3615 N3958 N3615 N3553 N4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21934</th>\n",
       "      <td>U21935</td>\n",
       "      <td>N1412 N1412 N2252 N2168 N1845 N2831 N2730 N209...</td>\n",
       "      <td>N3890 N3701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21935</th>\n",
       "      <td>U21936</td>\n",
       "      <td>N1448 N1448 N1519 N1536 N1519 N1517 N1845 N181...</td>\n",
       "      <td>N4274 N4053 N4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21936</th>\n",
       "      <td>U21937</td>\n",
       "      <td>N1379 N1379 N2035 N2071 N2451 N2514 N2252 N288...</td>\n",
       "      <td>N4125 N4053 N3862 N4274 N4375 N4549 N4555 N429...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21937</th>\n",
       "      <td>U21938</td>\n",
       "      <td>N1504 N1504 N1448 N1528 N1604 N1528 N1993 N216...</td>\n",
       "      <td>N3556 N4107 N3589 N3589 N3589 N4470 N4350 N443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21938</th>\n",
       "      <td>U21939</td>\n",
       "      <td>N1471 N1471 N1817 N1804 N1993 N1817 N2252 N229...</td>\n",
       "      <td>N3862 N4274 N4125 N3862 N3544 N4395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21939 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User                                            History  \\\n",
       "0          U1  N1 N1 N317 N329 N521 N533 N874 N830 N868 N933 ...   \n",
       "1          U2  N1 N1 N3 N98 N1 N329 N15 N385 N385 N817 N831 N...   \n",
       "2          U3  N1 N1 N387 N385 N329 N335 N496 N868 N1057 N830...   \n",
       "3          U4  N2 N2 N148 N206 N148 N335 N329 N408 N868 N879 ...   \n",
       "4          U5  N3 N3 N3 N230 N335 N329 N329 N426 N408 N432 N7...   \n",
       "...       ...                                                ...   \n",
       "21934  U21935  N1412 N1412 N2252 N2168 N1845 N2831 N2730 N209...   \n",
       "21935  U21936  N1448 N1448 N1519 N1536 N1519 N1517 N1845 N181...   \n",
       "21936  U21937  N1379 N1379 N2035 N2071 N2451 N2514 N2252 N288...   \n",
       "21937  U21938  N1504 N1504 N1448 N1528 N1604 N1528 N1993 N216...   \n",
       "21938  U21939  N1471 N1471 N1817 N1804 N1993 N1817 N2252 N229...   \n",
       "\n",
       "                                                Question  \n",
       "0                                                  N4296  \n",
       "1                          N3589 N3553 N3987 N4395 N4453  \n",
       "2                                                  N4296  \n",
       "3                                                  N4475  \n",
       "4        N3539 N3550 N3550 N3615 N3958 N3615 N3553 N4296  \n",
       "...                                                  ...  \n",
       "21934                                        N3890 N3701  \n",
       "21935                                  N4274 N4053 N4296  \n",
       "21936  N4125 N4053 N3862 N4274 N4375 N4549 N4555 N429...  \n",
       "21937  N3556 N4107 N3589 N3589 N3589 N4470 N4350 N443...  \n",
       "21938                N3862 N4274 N4125 N3862 N3544 N4395  \n",
       "\n",
       "[21939 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = pd.read_csv('data/user.tsv', sep='\\t', names=['User', 'History', 'Train', 'Question'])\n",
    "user['History'] = user['History'] + user['Train']\n",
    "user = user.drop(columns=['Train'])\n",
    "\n",
    "user[\"History\"] = user[\"History\"].apply(lambda x: \" \".join([item.split(\",\")[0] for item in x.split(\";\") if item]))\n",
    "user[\"Question\"] = user[\"Question\"].apply(lambda x: \" \".join([item.split(\",\")[0] for item in x.split(\";\") if item]))\n",
    "\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_news[history_news['News ID'].isin(history_news_ids)]['Category']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coongya11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
